# Cruise Has Hit an All-Time Low, New Boss Reportedly Tells Staff - Business Insider

## Unknown date - www.businessinsider.com

Source: https://www.businessinsider.com/cruise-self-driving-robotaxis-all-time-low-mo-elshenawy-staff-2023-12

- Cruise's new boss told staff the company was at an "all-time low," Reuters reported. - Mo Elshenawy said Cruise had taken the wrong approach to self-driving. - The company faces an uncertain future after recalling its entire fleet of driverless cars. Cruise's new boss reportedly told staff the company had hit an "all-time low" after recalling its entire fleet of driverless cars following a gruesome accident. In a company all-hands on Tuesday, Cruise president Mo Elshenawy told staff the company had taken the wrong approach to self-driving and lost the trust of stakeholders and regulators, a transcript seen by Reuters showed. It comes as the company faces a deepening crisis following an incident in San Francisco in October in which a pedestrian was dragged 20 feet under one of its robotaxis after it ran them over. "We went from an all-time high to an all-time low and from being an industry leader to temporarily pausing all of our operations," Elshenawy, who replaced Cruise cofounder Kyle Vogt as the company's leader after he resigned last month, reportedly told staff. "We now know that we need to be significantly better than human performance and significantly better across a much wider spectrum of use cases and edge cases," he said. Cruise's license to operate in California was revoked following the accident, with regulators accusing the company of failing to disclose video footage of the incident. The company paused its US robotaxi operations and later recalled all 950 of its self-driving cars to fix the issue behind the incident in October, during which the robotaxi attempted to pull over when the pedestrian was thrown into its path after being struck by another vehicle. Cruise faces fines and sanctions of up to $1.5 million for its handling of the crash. The driverless car company's future looks increasingly uncertain, with owners General Motors cutting spending on the division and layoffs planned for later this month. Elshenawy said Cruise had a long way to go to regain trust in its vehicles, with further reports emerging in recent weeks that Cruise's driverless cars struggled to detect children and large holes. He said the intense scrutiny had negatively impacted its employees. "This last week a Cruiser shared with me that they don't wear their Cruise jacket in public anymore. It truly breaks my heart," he said. Cruise declined to comment following a request from Business Insider.

---

# Autonomous vehicles and ADAS solutions | Google Cloud | Google Cloud

## Unknown date - cloud.google.com

Source: https://cloud.google.com/solutions/automotive/autonomous?hl=en

Accelerate the development and commercialization of autonomous vehicle (AV) and advanced driver assistance system (ADAS) solutions on scalable, open, and cost-efficient cloud infrastructure. Solutions Explore solutions for developing, deploying, and managing autonomous vehicle and ADAS applications at scale. Use Google Cloud’s high performance computing to power vehicle simulation workloads to generate, test, and validate millions of simulated miles. Learn how Cruise runs millions of self-driving car simulations using Google Cloud. Create a fully managed CI/CD pipeline with open source portability on a consistent platform that works across multiple deployment environments. Learn how Sibros makes it possible to build and launch new connected vehicle use cases from production to post-sale at scale. Proactively receive automated recommendations with Active Assist to help reduce costs, increase performance, improve security, and make more sustainable decisions. Explore how Nuro uses the Edge Appliance to rapidly transfer data to Cloud Storage. Simplify the data ingest process from on-prem to the cloud and in-vehicle computation leveraging Google Cloud-managed, secure, and high-performance appliance. See how Subaru reduces pre-processing time from days to minutes using Google Cloud and Dataflow. Cloud Storage has extended storage bucket locations to store data with high levels of availability and performance. Learn how Cruise uses Cloud Storage as a data lake for petabytes of on-the-road-data for quick and efficient ingestion. Vertex AI Neural Architecture Search (NAS) generates neural networks to help optimize model performance. Read how Nuro's perception team has accelerated their AI model development with Google Cloud and Vertex AI NAS. Empower ML and data science teams for rapid prototyping and model development using Vertex AI Notebooks and services. Support GPUs for distributed ML training clusters, reduce underutilized GPUs for inference workloads, and expand large-scale neural networks. See how Google Cloud can provide up to 16 A100 GPUs in a single VM, offering a total of 640 GB of GPU memory. Customers Developing autonomous technology is a battle with data. See how Google Cloud customers are solving the world's most complex challenges. Partners Our industry partners can help you solve your business challenges and unlock growth opportunities with painless implementations and integrated out-of-the-box or custom solutions. What's new Tell us what you’re solving for. A Google Cloud expert will help you find the best solution.

---

# Active Assist Cloud Management | Google Cloud

## Unknown date - cloud.google.com

Source: https://cloud.google.com/solutions/active-assist?hl=en

Catch up on the latest product launches, demos, and trainings from Next '23. Active Assist is a portfolio of intelligent tools that helps you optimize your cloud operations with recommendations to reduce costs, increase performance, improve security, and even help you make more sustainable decisions. Explore your recommendations and insights in-context on individual product pages, together in your Recommendation Hub, or through our Recommender API. Active Assist value categories and benefits Help you manage your cost wisely, such as recommending to delete unused or idle resources, downsizing VMs to fit your workload needs, or using committed use discounts to save money. Harden your security posture by applying recommended actions to reduce over-granted permissions, enable additional security features, and help with compliance and security incident investigations. Improve the performance of your cloud resources and workloads through prediction and automation that take your infrastructure one step ahead of what your applications need next. Increase the availability and reliability of your cloud resources and your workloads running on Google Cloud via various health checks, auto-scaling capabilities, and Business Continuity and Disaster Recovery options. Enhance your management experience on Google Cloud via simplification and automation so that you spend less time managing your cloud configuration and spend more time on innovating your digital businesses and delighting your customers. Offer you the insights and simple-to-use tools to allow you assess, manage, and reduce the carbon footprint of your workloads running on Google Cloud. Sign up for our Active Assist Trusted Tester Group to get early access to new features as they're developed. Start building on Google Cloud with $300 in free credits and 20+ always free products.

---

# How Cruise tests its AVs on a Google Cloud platform | Google Cloud Blog

## Unknown date - cloud.google.com

Source: https://cloud.google.com/blog/products/containers-kubernetes/how-cruise-tests-its-avs-on-a-google-cloud-platform

Building continuous integration & continuous delivery for autonomous vehicles on Google Cloud Mo Elshenawy Executive Vice President of Engineering, Cruise San Francisco’s roads are a notoriously tough place for people to drive. Narrow streets, busy pedestrian traffic, and an extensive network of bicycle routes make driving through San Francisco roughly 40 times more complex than driving in a suburban environment, according to Cruise data. This level of complexity makes San Francisco, America’s second densest city, an ideal environment for testing the most advanced self-driving technology found in today’s autonomous vehicles (AVs). Cruise has been focused on tackling city driving since 2013, and with more than 3 million autonomous miles under our belt, we are getting close to delivering an all-electric, shared, self-driving service. In January 2022 we reached a major milestone — members of the public took their first fully driverless rides in San Francisco. Developing an urban fleet of fully autonomous vehicles is an incredible challenge. To be successful, we needed to build both the AV tech that’s out on the road and the ecosystem of tools our engineers use internally. In creating this cycle, we thought carefully about how to best empower our engineers. Home-grown technology chains like this can get unwieldy quickly — the last thing you want as an engineer is to fight the disparate tools you are using or jump across too many of them to get your daily work done. Google Cloud has been a useful tool for us as we worked to streamline and support this development cycle, adapt our cloud-first infrastructure strategy, and seamlessly scale our systems. The frameworks we use in the vehicle, data logging, visualization, data mining, and ML modeling are all built on top of a common foundation that makes going from road to code — and from code to road — fast and safe. Let’s take a deeper look at how Google Cloud supports the pillars of our homegrown AV development cycle. The foundational platform At the base of this development cycle is a strong foundation of Google Kubernetes Engine (GKE) and Compute Engine virtual machines (VMs) with other Google Cloud Infrastructure Services such as Cloud Storage, CloudSQL, PubSub, Cloud Functions, BigQuery, App Engine, and Cloud Logging and Monitoring, etc. We chose to use Kubernetes (k8s) to build our multi-tenant, multi environment Platform as a Service, and build a homegrown tool we call “Juno” that helps Cruise application developers to iterate and deploy at scale instantaneously. On this Kubernetes and Istio-based Service platform (PaaS) we reuse a set of Kubernetes (k8s) operators/controllers to manage k8s clusters, manage nodepool migrations, and inject default add-ons for monitoring, logging, and service mesh. Under the hood we reuse Google Cloud’s Config Connector to manage dependencies while creating k8s resources. We run containerized workloads namespaced for multi-tenancy with high compute utilization and less operational overhead. We choose to isolate k8s clusters on the Service platform across development, staging, and production environments. We have also standardized continuous integration and continuous delivery to each of these environments. At the infrastructure level, we configured Google Cloud with vertical and horizontal boundaries. These boundaries and permissions, including Kubernetes namespace RBAC, Google Cloud projects and Google groups are managed centrally with our homegrown resource lifecycle manager, PaaS Juno. Vertically, we use Google Cloud folders and projects to separate each team’s development, staging, and production environments from other teams’. These environments each have their own VPC, subnets, firewalls, interconnects, NAT gateways, private DNS, and any other Google Cloud service that the application service may need, for example GKE/k8s, Compute Engine, Cloud Storage, Pub/Sub, etc. The services are connected by a network, but traffic between them can be easily audited and regulated. Horizontally, Google Cloud projects within each environment allow for distinct permission management and visibility constraints between tenants. Projects make it easy for tenants to focus on what matters to them. Separation between tenants makes it harder for malicious actors to gain unauthorized access, and prevents tenants from accidentally impacting one another. The ML Brain: ingestion, analysis and modeling With this strong foundation in place, we’re able to start feeding the pipeline. Every day, we traverse San Francisco’s city streets and collect a lot of anonymized road data. At a high level, we take all sensor data as input — from camera, lidar, audio to radar. From there, Cloud Storage and Dataflow help us create highly scalable ETL pipelines and data lakes to create and extract petabytes of on-the-road data quickly and efficiently. Leveraging BigQuery, few hundred engineers are able to process exabytes of data across more than 20 million queries every month. But even in San Francisco, less than 1% of the raw data contains useful information, and we quickly realized that we would need to distinguish the signal from the noise to make this work. At our scale, automation was the only answer, so we invested heavily in automatic AI-based event extraction and classification. Using advanced AI for scene understanding, today we can automatically label and tag many different classes of events, and then make them immediately available to every engineer. For example, we rely on panoptic segmentation to recognize trash bins and other objects along the side of the road; we use fine-grained classification to distinguish emergency vehicles from other cars; and we track objects across time and sensors to estimate their motions. After we ingest and analyze that data, it’s fed back into our dynamic ML Brain — a continuous learning machine that actively mines from the collected data to automatically train new models that exceed the performance of the older models. This is done with the help of Vertex AI, where we are able to train hundreds of models simultaneously, using hundreds of GPU years every month! Together, the automation, data sharing, and collaboration that Google Cloud’s tools provide supercharge Cruise engineers’ daily work. Testing and simulation: Building an AV “time machine” In addition to powering the ML Brain of the fleet, we also leverage our Google Cloud infrastructure to test/validate at scale, and build the confidence we need in order to release multiple times per week. No matter how many miles our fleet racks up on the road, there will always be unexpected moments that we won’t capture with road miles alone—we need to couple road data with robust simulation data to ensure a safe and comfortable ride for future customers. To make this happen, we developed a highly complex, distributed suite of testing and simulation tools that work in concert on top of Compute Engine. First, we rely on a tool we call “Morpheus” to generate specific testing scenarios quickly. Simulations are a scalable way to perform robust testing around rare and difficult events at any time of day, in any condition, and in any location. Need to test an AV driving down a misty one-way road at twilight? Or see what it’s like to crest a steep hill while driving east at sunrise? What about a rush-hour evening drive along the Embarcadero on game day? Morpheus automatically creates a realistic real world to “drive” through to maintain Cruise’s rigorous safety standards. To populate that world with unique driving scenarios we developed a tool called “Road-to-Sim” which fuses information from our millions of real-world miles to create a full-simulation environment from road data. This is an automated conversion pipeline that procedurally recreates any on-road event without manual intervention. This lets us easily create permutations of the event and change attributes like vehicle and pedestrian types. Simulation helps us prepare for geographic scale as well. We developed “WorldGen”, a system that can procedurally generate entire cities, allowing us to test in new operational design domains more efficiently and with a shorter lead time. By combining multiple sources of data, we build seamless map tiles across large geographic areas that can immediately be used for large-scale testing. At the root of this robust simulation engine is “Hydra”, our homegrown batch scheduling platform that scales up to tens of thousands of custom VMs that are optimized using Google Cloud's custom instances to run millions of simulations every day. While Hydra is a batch servicer, we have many types of batch workloads like simulation, ground truth labeling, and remote builds. All in all, Google Cloud’s highly elastic platform allows us to easily consume 10s of GPU years per day and 1000s of core years per day. In return, our simulation engine generates petabytes of synthetic data daily, which then is parsed, analyzed and fed back into our development cycle. This robust simulation and testing engine not only helps us scale and improve our current fleet of AVs, it also gives us a head start in testing and validating our next-generation vehicle: the Origin. Before the Origin ever hits the road, it will have been driving in simulation for years beforehand, encountering and learning from countless long-tail events in the cloud. Ensuring resilience amidst uncertainty In addition to supporting our continuous development cycle, Google Cloud has supported our infrastructure and engineering teams during a period of high growth and unprecedented challenges. Since early 2020, the Cruise Development Platform has proved to be resilient as our engineering team has nearly doubled in size, our workforce transitioned from in-office to work-from-home, and the tech industry has encountered unprecedented supply chain shortages. During this time, Google Cloud has enabled the entire organization to seamlessly scale: We grew our compute usage by 6x from the month preceding the pandemic to the present During this pandemic-era transition we tripled our daily compute usage We also tripled our total capacity enabling us to scale up by more than 150x our base usage in any given day. Now, we have the ability to serialize terabytes of data per minute. A strong and scalable infrastructure is critical for our ability to offer rides to paying customers on complex city roads. As we inch closer to that reality, Google Cloud gives us unprecedented speed and elasticity in our systems, and ensures the high level of safety our fleet needs. For more information on the complex chain of technologies powering Cruise AVs, watch our engineering showcase “Under the Hood.”

---

## Unknown date - cloud.google.com

Source: https://cloud.google.com/blog/products/containers-kubernetes/how-cruise-tests-its-avs-on-a-google-cloud-platform/

Building continuous integration & continuous delivery for autonomous vehicles on Google Cloud Mo Elshenawy Executive Vice President of Engineering, Cruise San Francisco’s roads are a notoriously tough place for people to drive. Narrow streets, busy pedestrian traffic, and an extensive network of bicycle routes make driving through San Francisco roughly 40 times more complex than driving in a suburban environment, according to Cruise data. This level of complexity makes San Francisco, America’s second densest city, an ideal environment for testing the most advanced self-driving technology found in today’s autonomous vehicles (AVs). Cruise has been focused on tackling city driving since 2013, and with more than 3 million autonomous miles under our belt, we are getting close to delivering an all-electric, shared, self-driving service. In January 2022 we reached a major milestone — members of the public took their first fully driverless rides in San Francisco. Developing an urban fleet of fully autonomous vehicles is an incredible challenge. To be successful, we needed to build both the AV tech that’s out on the road and the ecosystem of tools our engineers use internally. In creating this cycle, we thought carefully about how to best empower our engineers. Home-grown technology chains like this can get unwieldy quickly — the last thing you want as an engineer is to fight the disparate tools you are using or jump across too many of them to get your daily work done. Google Cloud has been a useful tool for us as we worked to streamline and support this development cycle, adapt our cloud-first infrastructure strategy, and seamlessly scale our systems. The frameworks we use in the vehicle, data logging, visualization, data mining, and ML modeling are all built on top of a common foundation that makes going from road to code — and from code to road — fast and safe. Let’s take a deeper look at how Google Cloud supports the pillars of our homegrown AV development cycle. The foundational platform At the base of this development cycle is a strong foundation of Google Kubernetes Engine (GKE) and Compute Engine virtual machines (VMs) with other Google Cloud Infrastructure Services such as Cloud Storage, CloudSQL, PubSub, Cloud Functions, BigQuery, App Engine, and Cloud Logging and Monitoring, etc. We chose to use Kubernetes (k8s) to build our multi-tenant, multi environment Platform as a Service, and build a homegrown tool we call “Juno” that helps Cruise application developers to iterate and deploy at scale instantaneously. On this Kubernetes and Istio-based Service platform (PaaS) we reuse a set of Kubernetes (k8s) operators/controllers to manage k8s clusters, manage nodepool migrations, and inject default add-ons for monitoring, logging, and service mesh. Under the hood we reuse Google Cloud’s Config Connector to manage dependencies while creating k8s resources. We run containerized workloads namespaced for multi-tenancy with high compute utilization and less operational overhead. We choose to isolate k8s clusters on the Service platform across development, staging, and production environments. We have also standardized continuous integration and continuous delivery to each of these environments. At the infrastructure level, we configured Google Cloud with vertical and horizontal boundaries. These boundaries and permissions, including Kubernetes namespace RBAC, Google Cloud projects and Google groups are managed centrally with our homegrown resource lifecycle manager, PaaS Juno. Vertically, we use Google Cloud folders and projects to separate each team’s development, staging, and production environments from other teams’. These environments each have their own VPC, subnets, firewalls, interconnects, NAT gateways, private DNS, and any other Google Cloud service that the application service may need, for example GKE/k8s, Compute Engine, Cloud Storage, Pub/Sub, etc. The services are connected by a network, but traffic between them can be easily audited and regulated. Horizontally, Google Cloud projects within each environment allow for distinct permission management and visibility constraints between tenants. Projects make it easy for tenants to focus on what matters to them. Separation between tenants makes it harder for malicious actors to gain unauthorized access, and prevents tenants from accidentally impacting one another. The ML Brain: ingestion, analysis and modeling With this strong foundation in place, we’re able to start feeding the pipeline. Every day, we traverse San Francisco’s city streets and collect a lot of anonymized road data. At a high level, we take all sensor data as input — from camera, lidar, audio to radar. From there, Cloud Storage and Dataflow help us create highly scalable ETL pipelines and data lakes to create and extract petabytes of on-the-road data quickly and efficiently. Leveraging BigQuery, few hundred engineers are able to process exabytes of data across more than 20 million queries every month. But even in San Francisco, less than 1% of the raw data contains useful information, and we quickly realized that we would need to distinguish the signal from the noise to make this work. At our scale, automation was the only answer, so we invested heavily in automatic AI-based event extraction and classification. Using advanced AI for scene understanding, today we can automatically label and tag many different classes of events, and then make them immediately available to every engineer. For example, we rely on panoptic segmentation to recognize trash bins and other objects along the side of the road; we use fine-grained classification to distinguish emergency vehicles from other cars; and we track objects across time and sensors to estimate their motions. After we ingest and analyze that data, it’s fed back into our dynamic ML Brain — a continuous learning machine that actively mines from the collected data to automatically train new models that exceed the performance of the older models. This is done with the help of Vertex AI, where we are able to train hundreds of models simultaneously, using hundreds of GPU years every month! Together, the automation, data sharing, and collaboration that Google Cloud’s tools provide supercharge Cruise engineers’ daily work. Testing and simulation: Building an AV “time machine” In addition to powering the ML Brain of the fleet, we also leverage our Google Cloud infrastructure to test/validate at scale, and build the confidence we need in order to release multiple times per week. No matter how many miles our fleet racks up on the road, there will always be unexpected moments that we won’t capture with road miles alone—we need to couple road data with robust simulation data to ensure a safe and comfortable ride for future customers. To make this happen, we developed a highly complex, distributed suite of testing and simulation tools that work in concert on top of Compute Engine. First, we rely on a tool we call “Morpheus” to generate specific testing scenarios quickly. Simulations are a scalable way to perform robust testing around rare and difficult events at any time of day, in any condition, and in any location. Need to test an AV driving down a misty one-way road at twilight? Or see what it’s like to crest a steep hill while driving east at sunrise? What about a rush-hour evening drive along the Embarcadero on game day? Morpheus automatically creates a realistic real world to “drive” through to maintain Cruise’s rigorous safety standards. To populate that world with unique driving scenarios we developed a tool called “Road-to-Sim” which fuses information from our millions of real-world miles to create a full-simulation environment from road data. This is an automated conversion pipeline that procedurally recreates any on-road event without manual intervention. This lets us easily create permutations of the event and change attributes like vehicle and pedestrian types. Simulation helps us prepare for geographic scale as well. We developed “WorldGen”, a system that can procedurally generate entire cities, allowing us to test in new operational design domains more efficiently and with a shorter lead time. By combining multiple sources of data, we build seamless map tiles across large geographic areas that can immediately be used for large-scale testing. At the root of this robust simulation engine is “Hydra”, our homegrown batch scheduling platform that scales up to tens of thousands of custom VMs that are optimized using Google Cloud's custom instances to run millions of simulations every day. While Hydra is a batch servicer, we have many types of batch workloads like simulation, ground truth labeling, and remote builds. All in all, Google Cloud’s highly elastic platform allows us to easily consume 10s of GPU years per day and 1000s of core years per day. In return, our simulation engine generates petabytes of synthetic data daily, which then is parsed, analyzed and fed back into our development cycle. This robust simulation and testing engine not only helps us scale and improve our current fleet of AVs, it also gives us a head start in testing and validating our next-generation vehicle: the Origin. Before the Origin ever hits the road, it will have been driving in simulation for years beforehand, encountering and learning from countless long-tail events in the cloud. Ensuring resilience amidst uncertainty In addition to supporting our continuous development cycle, Google Cloud has supported our infrastructure and engineering teams during a period of high growth and unprecedented challenges. Since early 2020, the Cruise Development Platform has proved to be resilient as our engineering team has nearly doubled in size, our workforce transitioned from in-office to work-from-home, and the tech industry has encountered unprecedented supply chain shortages. During this time, Google Cloud has enabled the entire organization to seamlessly scale: We grew our compute usage by 6x from the month preceding the pandemic to the present During this pandemic-era transition we tripled our daily compute usage We also tripled our total capacity enabling us to scale up by more than 150x our base usage in any given day. Now, we have the ability to serialize terabytes of data per minute. A strong and scalable infrastructure is critical for our ability to offer rides to paying customers on complex city roads. As we inch closer to that reality, Google Cloud gives us unprecedented speed and elasticity in our systems, and ensures the high level of safety our fleet needs. For more information on the complex chain of technologies powering Cruise AVs, watch our engineering showcase “Under the Hood.”

---

# 3 Ways Cruise HD Maps Give Our Self-Driving Vehicles An Edge | by Erin Antcliffe | Cruise | Medium

## Unknown date - medium.com

Source: https://medium.com/cruise/hd-maps-self-driving-cars-b6444720021c

3 Ways Cruise HD Maps Give Our Self-Driving Vehicles An Edge Especially when it comes to complex urban environments like San Francisco. Written by Erin Antcliffe, Senior Product Manager, Mapping At Cruise, we’re on a mission to build the world’s most advanced self-driving vehicles to safely connect people with the places, things and experiences they care about. The potential to save millions of lives, reshape our cities, give people more time, and restore freedom of movement for many motivates our teams at Cruise everyday. My team, the Mapping team, creates maps that store important pre-computed information about the world. This allows our self-driving vehicles to focus on other roadway users. High Definition (HD) maps serve as one part of the self-driving vehicle’s “eyes”. HD Maps are made up of two main types of assets: - 3D tiles that are rendered from LiDAR sensor data - Semantic labels that are applied on top of this sensor data to give meaning to the environment HD maps provide self-driving vehicle with information about real-world environmental features, such as the boundaries of lanes, location of traffic lights, and presence of curbs at the edge of the roadway. All of this information lightens the real-time processing load on the self-driving vehicle, letting it focus on navigating around dynamic actors in the roadway, such as vehicles, bicycles and pedestrians. As the autonomous vehicle industry has grown over the past few years, companies have had to decide whether to create and maintain their own HD maps or use maps from one of the many HD mapping companies that have evolved to support the industry. At Cruise, we’ve decided to build our own maps to maintain full control over the format, quality, and taxonomy. This allows us to operate more successfully in complex urban environments, keep our maps up-to-date with real-world changes, and facilitate rapid self-driving development. Precise maps help self-driving vehicles localize in complex urban environments In today’s smartphone era, you’ve likely used GPS on your phone to find out where you are and where you’re headed. If you’ve tried this in a city as complex as San Francisco, you may have encountered an inaccurate location reading that tells you you’re several blocks away from where you actually are. This is because tall buildings like the Transamerica pyramid obscure the line of sight between your phone and GPS satellites, leaving you with low confidence in your position (this is known as the urban canyon effect). This inaccuracy might be fine if you are able to figure out your location relative to where you’re trying to go on foot. But for a self-driving vehicle, this low level of positional accuracy is paralyzing. To solve for this, self-driving vehicles use LiDAR sensors to compare their surrounding environment with the 3D map and determine their location down to centimeter-level accuracy. This allows us to perform sensitive maneuvers with a high degree of confidence by enabling the car to make sense of its surroundings and plan it’s next action accordingly, such as moving past a bicycle in the next lane or anticipating pedestrians crossing the street at an upcoming mid-block crosswalk. Updating maps with San Francisco’s constantly changing streets As I described above, HD maps provide a critical input to self-driving vehicles. Given this fact, it’s important to keep the map up-to-date with changes out on the road. Dense urban areas like San Francisco are constantly undergoing construction projects like adding protected bike lanes, equipping better traffic control signals, and building new areas of residential or commercial development. We have developed sophisticated product and operational solutions to detect real-world changes and send map updates to every autonomous vehicle in the fleet in minutes. This is one of the reasons why maintaining our own maps is a competitive advantage. Facilitating rapid self-driving development Another major advantage to producing our own maps is the ability to leverage our map production platform to experiment more quickly on cutting-edge autonomous feature development. Just like human drivers, self-driving vehicles perform better when they are familiar with the road environment and expected behavior of other vehicles (e.g. encountering an all-way stop intersection). We can give our fleet an edge by using maps to encode this information based on thousands of interactions in a given area. As we experiment with different ways to represent spatial data, we can quickly test our hypotheses, iterating on different versions of a new map feature to assess its impact on our self-driving vehicle’s performance. Once we find the right solution, our map production system allows us to quickly scale the new feature across the entire map. By working closely together with Cruise Engineering teams to develop map features side-by-side with self-driving behavior improvements, we can more quickly deliver on our vision of a fully self-driving vehicles. Cruise maps are a foundational advantage to self-driving development With the wide variety of challenging scenarios in San Francisco’s complex urban environment, HD maps are a tool that give self-driving vehicles an edge when driving. At Cruise, one of our fundamental advantages comes from producing our own HD maps using precision LiDAR and semantic mapping techniques. By building maps in-house, we maintain full control over the maintenance strategy and can quickly iterate on new map features to help self-driving vehicles get on the road more quickly. Overall, Cruise maps empower our self-driving vehicles to drive in SF so we can achieve our mission of building the world’s most advanced self-driving vehicles to safely connect people with the places, things and experiences they care about. The sooner we deliver all-electric, self-driving vehicles, the sooner we can save lives, reduce emissions, take back our time, and democratize transportation. Join the self-driving team If you want to play a part in achieving a self-driving future, join us! You can find my team under the department, “Product Management & Design.” See you on the road!

---

# Building Self-Driving Hardware at Scale | by Brendan Hermalyn | Cruise | Medium

## Unknown date - medium.com

Source: https://medium.com/cruise/building-self-driving-hardware-at-scale-29589d2b4a09

Building Self-Driving Hardware at Scale A famous and frequently paraphrased truism is that if you’re really serious about software, you should make your own hardware. At Cruise, we’re serious about both. I lead the Autonomous Vehicle Hardware Systems organization here at Cruise. Our team designs, develops, and commoditizes the hardware and architectures — such as sensors, compute, and associated subsystems — in support of Cruise’s business needs. Working tightly with our software, systems, and product customers, our team develops modular, safe, and scalable architectures to support our vehicle platforms and business cases and brings them through production with our world-class partners. In this blog post, we’ll introduce you to the hardware systems team at Cruise, our philosophy on hardware development, and how we deliver self-driving hardware at scale. Our hardware development philosophy at Cruise Given the state of the market and outsized publicity that new entrants and announcements attract, I am often asked: “What are the ‘right’ sensors or computers for self-driving cars?” The practical reality is that all of the groups currently grappling with this autonomous challenge are essentially solving the same physics problems of sensing, perception, processing, and kinematics — and there are several ways that you can attack these problems. Military and aerospace-grade sensor and compute packages have been deeply developed over the last century and are remarkably effective at sensing the environment — but at a cost point that is prohibitive to the massive scale required to achieve our goals in the self-driving space. Autonomous trucks have been operating for years in industrial mining applications under similar constraints. But there is a cleverness required to build a system that can conduct real-time, safety-critical sensing and perception tasks at scale. I think this is a remarkably exciting problem that can have a profound impact on not only how we get around, but also potentially reducing the number of needless deaths and injuries that occur daily on our roads. The answer to the “right hardware” question is therefore a little nuanced — the best hardware for self-driving cars is not necessarily the best sensor or compute, but rather the best system. The right perception and processing system must be tuned to the specific software used to meet the stringent performance, safety, cost, scale, and reliability requirements of this space. It is indeed an exciting time in this industry with the emergence of a large number of burgeoning sensor and technology startups/companies. So why invest in making hardware at all? Can’t you just buy everything you need? While an amazing amount of development has occurred across the industry, significant amounts of work are still required in order to truly commoditize a hardware system of sufficient performance at the scale required by the business case. The initial conditions of each self-driving company — such as the time, environment, and background — play a critical role in the shape of the hardware and software stack. Thanks to developments in the sensor market, autonomous vehicle companies that began tackling this problem in the last few years have a huge temporal advantage — you can simply buy a self-driving kit and get started. This was the field that Cruise started out with several years ago, and these initial conditions allowed the software development to proceed unencumbered by the long and expensive hardware development lifecycle. As the software matures, the hardware can become more specialized and optimized for the system. To be successful in self-driving, your hardware needs to be successful along three axes at once: - Safety. A broad word that distills many concepts, including functional safety, system performance, reliability, and graceful degradation. Performance requirements must be met to succeed. This includes not only piece part cost but also development, maintenance and reliability. Only a few companies are able to exert the resources — monetary and otherwise — to go after the difficult hardware projects that are required to make a meaningful disruptive impact on the hardware landscape for autonomous vehicles. For us at Cruise, those hard problems are existential to our business. On the other side of the coin, one can go too far in vertical integration. We at Cruise strive to leverage developments in the industry as much as we can, and focus our investments on the components and technologies that offer outsized value for our mission- the “minimum necessary” technologies core to our self-driving system. That minimum is still challenging. At each step of the hardware development cycle, we must conduct a make/buy study. Of the over 30 modules that are custom for the autonomous vehicles that you can currently see driving around the streets of San Francisco, about two-thirds are fully bespoke; this percentage is closer to 75% for our next iteration of vehicle. These components include things that you would expect — like fully custom cameras and radars and our compute modules, but also many of the back end telematics, networking, and interface modules as well. For the remainder, we deeply partner with the companies making these components to co-develop them to our specifications and needs. Developing self-driving cars at scale It is extremely easy and common to underestimate the difficulty of scale and reliability in the face of a daunting performance challenge. However, much like driving in difficult weather, these manufacturing hurdles cannot be “added on” once the system works; instead, they must be designed in from the start on both system and component levels. There is also an additional element to this equation: the ability to iterate rapidly without sacrificing any of our performance metrics. These areas are where our deep partnership with GM is crucial. Current Cruise self-driving cars are based on the Chevy Bolt platform, but are executed as a different, unique vehicle within the GM system to enable them to roll off the line hardware complete — which required a significant redesign of the production process. While traditional automotive manufacturing typically requires a multi-year process to develop a vehicle, that timeline is too slow to respond to our iterations and keep up the momentum for advancing this technology. Over several years of working together, we have consciously developed teams and processes to impedance match the safety and manufacturing mindset of one of the world’s biggest automakers with the high speed and rapid iteration of Silicon Valley. This partnership is not an easy thing to develop, and most certainly does not come for free with an OEM partnership; we have had to practice to get good at it. Each major vehicle revision is considered a new “track” of vehicles — the track you can see driving around San Francisco is the third iteration with GM (“Track 3”). If you look very carefully, however, nearly every sensor and almost all autonomous vehicle components were either modified, replaced, repositioned, or updated (and sometimes, all of the above) between the beginning of Track 3 about a year and a half ago and the current vehicles. What’s more, all of them have rolled off the factory line without sacrificing the reliability of our components or systems — a critically important detail. This allows us to rapidly switch out and modify systems on our production vehicles in response to the availability of better components and to continued learning at the system level. We are able to accomplish this by keeping flexibility and modularity in key strategic areas of the vehicle, and by the nature of our end-to-end control of the whole vehicle from the design phase to the production line. While there is no replacement for actual, real world testing — and we do quite a bit of that — we have also invested significantly in our tools and simulation to enable extremely fast (yet reliable) development, evaluation and optimization of new sensor architectures. These include not only high-fidelity, photorealistic 3D simulations, which enables us to run our full software stack on proposed sensor layouts to check efficacy, but also in physics modeling of electromagnetic propagation to answer difficult placement and weather questions. This brings us full loop to our hardware development philosophy above and enables us to quickly identify what are the right things to develop. We are just beginning to see the fruits of this labor with some of our next-generation vehicles, and we envision this philosophy will become industry standard as we move from point designs to architectures on the way to solving the engineering challenge of our generation. Join Cruise’s hardware team Come join us build the future. You can find my team under “Engineering — Hardware”.

---

# Automating Firmware Security with FwAnalyzer | by Collin Mulliner | Cruise | Medium

## Unknown date - medium.com

Source: https://medium.com/cruise/firmware-security-fwanalyzer-dcbd95cef717

Automating Firmware Security with FwAnalyzer At Cruise, we aim to improve our security posture to build the most secure product possible. One major aspect of this goal is securing the various independent components of our all-electric vehicles. These components house everything from the autonomous driving functionality, external communications, and customer facing interfaces. Needless to say, securing these components is highly prioritized and requires continuous attention. The core of these components is their firmware. In this blog post and upcoming talk, Come Join the CAFSA — Continuous Automated Firmware Security Analysis at Black Hat USA 2019, we present our efforts around continuous firmware security analysis. The source code for our FwAnalyzer tool is available on Cruise’s GitHub page. The slides will also be made available via GitHub. Firmware Lifecycle Firmware, like any product, has a lifecycle from inception and prototyping to full force engineering and deployment to production. Each step in the lifecycle has a different focus and will likely impact the security of the firmware and final device in various ways. In addition, many devices are developed by a third party, not in-house. As a result, you have insights and control over fewer steps in the process. We identified three critical parts of the lifecycle to achieve the best possible outcome for security: - Development — specifically the everyday CI-based engineering effort - (Acceptance) Testing — especially for firmware delivered by third parties - Production Release — security as gatekeeper for production release and signing With this in mind, we developed processes and tools to efficiently analyze the firmware for our devices. As you will see, our processes and tools are significantly different compared to traditional methods, such as vulnerability scanners or CVE enumeration. Introducing FwAnalyzer We developed FwAnalyzer, a tool to analyze firmware for security issues. FwAnalyzer specifically targets filesystem images and, therefore, targets devices that have a clear separation between OS, applications, and filesystem. FwAnalyzer was built for Linux-based devices, including Android, but can be augmented to work on similar platforms. How FwAnalyzer works FwAnalyzer examines a filesystem image by applying a set of rules from a configuration file. The result of the analysis is returned as a machine readable report. Targeting filesystem images is the logical choice since they provide context, combine metadata such as pathnames, ownership, and permissions, along with the actual file content. Analysis of a standalone file cannot provide the same insights. FwAnalyzer implements a file system abstraction layer through which the analyzer functionality is implemented independently from the filesystem type. Adding support for new filesystem types is fairly easy. FwAnalyzer currently supports ext2/3/4, VFat, squashfs, UBIFS, and files in a local directory. Before FwAnalyzer can start its work, the filesystem images have to be extracted from a firmware update file. This step is not part of FwAnalyzer since it solely depends on the specific device, and thus will not be further discussed in this blog post. However, our open source release includes an unpacking and checking script as well as example configuration for Android firmware. This sample script nicely shows how to unpack and check a firmware image end-to-end. Configuring rules with FwAnalyzer Once the filesystem images are extracted, it is time to configure the rules that are to be applied. FwAnalyzer implements three different types of rules. Rules that are applied to file metadata such as permissions, type and ownership. Access to file metadata allows implementing checks around access control — a fundamental security control on any system. FwAnalyzer provides support for SELinux labels and could be extended to provide support for other extended attributes. Rules that target the content of a file. Analyzing the contents of a file is an obvious feature. In fact, a large number of our checks target the file content. Overall, we designed our file content analysis rules to be easy to understand and use. We implemented a number of checks that each provide different insights into file content. These checks will be described in more detail later in this blog post. Rules that analyze filesystem metadata. This includes things like whether a specific file exists or not, as well as whether a directory contains specific files. Furthermore, it provides the capability to compare the metadata of two filesystem images to determine if files were added, removed, or modified. This functionality can be used to compare two different versions of the same filesystem image. In addition to rules that perform a check of some kind, FwAnalyzer implements a data extraction subsystem. This gives FwAnalyzer the ability to include data extracted from files on the filesystem in a detailed report. The extracted data can provide valuable information such as versioning or configuration details. These details provide precise information about the specific firmware and can be used to implement more complex checks at a later phase of your security pipeline. Rules and Examples One of the most simple rules flags setuid (SUID) files to help identify possibly dangerous executables. SUID executables, often used to provide elevated privileges for specific actions, are notorious for local privilege escalation vulnerabilities, especially if those applications are developed in-house without going through proper security design and implementation review. The rule also allows ignoring certain known good SUID files such as common system utilities. Similarly, checking if a file exists can reveal that debugging and development binaries are present in the filesystem. Real world examples include CVE-2018–5399, where an ssh service was found on a production firmware and was removed in later firmware version. Analysis of file content is the core of FwAnalyzer. There are a number of types of file content checks. Below we present the three most important mechanisms: First, you can compare the SHA-256 digest of the file with a preconfigured value. This allows us to check that a specific file has specific content. This can be useful in the context of development vs. production public keys and certificates stored on the filesystem. Second, you can apply a regular expression to the contents of a file. This mechanism can be used to check configuration files to ensure certain settings are present. Similarly, release information stored on the filesystem can be checked to ensure the specific firmware update file was the result of a production built. One example would be Android properties such as ro.build.type that clearly indicates the type of build. For more complex checks, FwAnalyzer supports running an external script or binary to analyze the content of a file. For this, the file is extracted to a temporary location so the external script does not need to understand or care about anything besides the file it analyzes. External scripts enable easy extension of FwAnalyzer without modifying FwAnalyzer itself. Using FwAnalyzer to prevent security issues Armed with these capabilities, you now have the ability to detect and prevent a wide variety of security issues. Using an external script that we provide, you can detect any non-stripped binaries, preventing leaking potentially valuable debug information. You can ensure that files have the right permissions, so you do not end-up with issues such as CVE-2014–5457 where a password file is world readable. Further, you can use FwAnalyzer to prevent shipping firmware that contains private key files such as was seen in CVE-2017–8222. These are just a few examples of what FwAnalyzer can do. Overall checks will be contingent on the product and its functionality, but the underlying mechanisms provided by this tool are universal. Continuous Firmware Security FwAnalyzer is built to integrate into the development, test, and release processes. It is intended to be used for automation and produces machine readable output. At Cruise, we integrated FwAnalyzer into our development process as a check in CI, providing direct feedback during development and runs as one part of the standard unit tests. During CI, we use a different set of configurations for FwAnalyzer, depending on the build and signing type. This allows us to enforce more stringent security rules for production builds while not hindering development builds by being overly strict. In addition, this also helps to keep the balance between security and ease of development. Examples of differences would include stricter firewall rules and flagging specific binaries for production builds. For example, we may allow debugging tools or ssh for development builds, but not production builds. We also operate an analysis service for firmware images delivered to us from third parties. The service analyzes firmware and provides feedback to us and the supplier. Adding a new device to FwAnalyzer requires a one-time investigation and the creation of a corresponding FwAnalyzer configuration. Afterward, only minimal maintenance is required to support future releases of firmware for that device. Maintenance usually is required if the firmware update contains major changes. While the service is just one part of the acceptance testing process, it helps to speed it up due to being fully automated. FwAnalyzer and production signing Finally, we use FwAnalyzer as a gatekeeper for production signing of firmware. Production signing is an important security control since it is the step that enables a specific firmware image to be deployed into production. FwAnalyzer guards the production signing in multiple ways. Most importantly, it analyzes the firmware image and only allows it to be signed if it passes all checks. This step should eradicate accidental signing of bad firmware, such as development builds or engineering builds created by an individual developer. After all, we are all human and humans can make mistakes, especially when deadlines are close. By only production signing firmwares that have passed analysis, we eliminate these types of errors. Further, the analysis report generated by FwAnalyzer serves as a permanent record for each specific firmware image, especially when using the data extraction subsystem to its full extent. One main feature of FwAnalyzer’s JSON formatted report is that it can be used to further automate processes around security and security Q&A. At Cruise, we upload all FwAnalyzer reports to our central logging service. Using the logging service’s filters and triggers, we implement a variety of reporting and notifications using common off-the-shelf, cloud-based services. One simple example of this is having all analysis reports easily searchable. This allows one to easily access a report by searching using the version or the digest of the firmware in question. Automating Firmware Security is a step in the right direction FwAnalyzer has provided valuable results to the Cruise security teams on multiple occasions, some of which would otherwise have been very costly. It also enables us to quickly engage developers and third parties shortly after receiving new firmware revisions. After discovering issues, we are able to quickly implement new checks to automatically flag the issue in the future and effectively prevent regressions. We strongly believe that automation for firmware security is a big step in the right direction, especially when dealing with a large number of different devices and firmware. FwAnalyzer was designed to be easily extendable to add support for additional filesystem types, as well as adding new checks. The FwAnalyzer configuration file format is designed to allow including and sharing of rules between build types, devices, and even across organizations. Firmware security is extremely important not only for us at Cruise, but for every one who builds products. We decided to open source FwAnalyzer to help others to build more secure products as well. In addition to releasing FwAnalyzer completely open source under the Apache license, we will also include a number of example configuration files, scripts to perform various checks, as well as an example end-to-end unpacking and checking script for Android. If you are interested in making bespoke tools of your own to help solve the engineering challenge of our generation and potentially open source, take a look at our open roles.

---

# What enterprises can learn about data infrastructure from Cruise driverless cars | VentureBeat

## Unknown date - venturebeat.com

Source: https://venturebeat.com/data-infrastructure/what-enterprises-can-learn-about-data-infrastructure-from-cruise-driverless-cars/

Join our daily and weekly newsletters for the latest updates and exclusive content on industry-leading AI coverage. Developing safe driverless car technology is a highly specialized, complex and multifaceted undertaking — I know this firsthand, having recently worked for one of the small number of companies active in the sector. Despite that, there are many lessons that enterprises across industries can learn from the driverless car industry, especially companies moving to embrace generative AI. Not least among them: How to build a robust and secure data infrastructure to support their AI models, according to Mo Elshenawy, executive vice president (EVP) of engineering at Cruise, General Motors’ (GM) self-driving car subsidiary. “Data is the lifeline, and you work backward from there,” Elshenawy told me during our fireside chat at the VentureBeat Transform 2023 conference on Wednesday. “You’re going to find different [data] consumers across your organizations. Who needs the data and in what format they need it, and for how long? How soon do they need the data? So that’s a very important aspect to think about.” >>Follow all our VentureBeat Transform 2023 coverage<< Elshenawy shared his view from under the hood at Cruise, which launched the first customer-facing driverless car service in a major city — San Francisco — in early 2022. Today, Cruise’s driverless Chevy Bolts are a common sight in the City by the Bay, operating 24/7, though they are for now limited to those who have signed up for Cruise’s waiting list. Cruise handles more data than most organizations across all types of sectors, giving the company a unique vantage point for what works in terms of data infrastructure, data pipelines and stress tests. “Any given month, our Cruise engineers would be siphoning through some seven exabytes of data — equivalent to 150 million years of video streaming,” Elshenawy said. As such, Cruise had to make sure its data infrastructure was robust enough to handle this incredible volume of data, but also smart enough to categorize it and make it easily accessible to those in the company who needed to access it — all while maintaining high, safety-critical security. With vehicles capturing massive volumes of sensor data in real time, Cruise had to architect a data infrastructure from scratch that could handle the immense scale. Key considerations included scalability, security, cost optimization, and tooling to help engineers effectively leverage the data. From data lake to warehouse and lakehouse architecture One of the most pressing questions facing any organization looking to use generative AI — or those dealing with any kind of software and digital data, in fact — is where and how to store all their data. In the early days of personal computing and enterprise tech, digital “warehouses” were the answer. This meant putting structured data — organized data such as a spreadsheet, comma-separated-values file, or similar — into one system for keeping track of it all. But as organizations began to collect and sought to analyze more unstructured data — such as customer interactions, code, and multimedia content like photos, videos and audio — it became incumbent upon them to find another way to store it all, especially given the vast and rapidly increasing quantities they were accumulating. That was how the data lake was born. Finally, in the last few years, companies have moved to a hybrid data storage and retrieval architecture: the lakehouse architecture, which combines qualities of both structured and unstructured data and allows both types to be stored and retrieved in the same database. Elshenawy said Cruise’s own data infrastructure journey actually followed the inverse of this trend, beginning with a data lake and adding a warehouse and a lakehouse as the company moved from coding to testing to public-facing driverless cars on public roads. “At one point, in our life stage, it made perfect sense for us to just rely on a data lake because our main customers were our ML [machine learning] engineers,” Elshenawy said. “Then you move into another architecture, data warehouses. If you have a lake and a warehouse, you’re moving data around from one place to another. And once you get to that point, and you have like a two-tier data architecture, where you’re replicating your data, know for sure that you probably want to move into the new architecture of a lakehouse, where you still have one data lake, but you get the benefits of building a data warehouse on top of that, so you end up serving both customers really well.” He advocated that organizations in other industries approach their tasks with a similarly flexible mentality, beginning with only the data infrastructure they need and changing it as the organization grows, or if members of the organization need different types of data infrastructure to accomplish the organization’s goals. “You have ML engineers expecting streaming directly from a data lake, versus business intelligence analysts, they want a data warehouse.” Making sure your AI models don’t overfit or underfit the real-world use cases Though Cruise is not primarily in the business of developing, nor using, large language models (LLMs) such as Anthropic’s Claude 2 or OpenAI’s Chat GPT, Elshenawy did say there was one major challenge that LLM users and Cruise’s autonomous vehicle AI models shared: making sure the models don’t overfit or underfit — that is, that they are trained appropriately to respond to new, real-world data that they encounter that does not necessarily resemble their training data. This may include edge cases. Underfitting is when the AI model did not learn well enough from the data it was trained on to recognize patterns, and is not able to produce the desired responses reliably when encountering real-world use case data that closely resembles the training data — no matter what the sector or industry. Overfitting is when the AI model learned too well from the training data, and is flummoxed by new real-world data that does not match it, such as an edge case — an unusual event that does not happen frequently. The goal in the case of Cruise and those that use LLMs is to have an AI that is neither underfitted nor overfitted for its specific use case. Elshenawy said Cruise accomplishes this through the use of several different data science and machine learning techniques, including data augmentation and synthetic data generation. Drilling down specifically on augmentation, Elshenawy provided the example of Cruise cars currently testing by performing driverless trips in San Francisco on public roads. “Because we’re starting with San Francisco … we see a lot of great odd things that happen” while driving around, Elshenawy explained. “You can take one of those examples and create thousands of variations [in software] … change lighting conditions, the angles, speed velocities of all the other vehicles and so on. So you create almost a new dataset augmented out of something that you saw.” One odd thing that has been happening more frequently recently: protestors putting traffic cones on Cruise and Alphabet-backed rival Waymo’s driverless vehicles that are both testing in San Francisco, covering their sensors and causing them to stop in their tracks. Elshenawy said that even though these protests are a kind of “edge case,” the Cruise AI models had been built resiliently enough to act safely even when these incidents occur. “That is an example where actually our vehicles handle the situations very well because we’ve built a generalized model, and the safe thing if you cover a sensor or damage a sensor is for the vehicle to pull over, and and wait for someone to come in and clear that hazard.” AI + LLM = AGI? When asked about the prospect of combining autonomous driving systems with large language models (LLMs) to produce artificial general intelligence (AGI) Elshenawy was skeptical. “I don’t think putting them together with directly lead to artificial general intelligence. Both are great in their own methods. Putting them together can have great advancements in human-robot interactions, but it’s not generally going to lead to that … what I’m excited about is how quickly both of them advance.” Elshenawy also provided insight into Cruise’s rigorous approach to cybersecurity, essential for a safety-critical autonomous system. “You truly need a multidisciplinary team, a team that spans across software engineers, data engineers, analysts, data scientists, security engineers,” he said. The session offered a fascinating insider perspective on the data challenges overcome by one of the leaders in autonomous vehicles. As AI permeates more aspects of business and society, Cruise’s lessons on robust data infrastructure will only grow more relevant.

---

# Tech Icon Spotlight: Mo Elshenawy - Riviera Partners

## Unknown date - www.rivierapartners.com

Source: https://www.rivierapartners.com/tech-icon-spotlight-mo-elshenawy/

Tech Icon Spotlight: Mo Elshenawy EVP of Engineering at Cruise Mo Elshenawy remembers learning to play chess as a young boy using an electronic chess board. After entering his move, the chess board would tell him which piece it wanted to move and where to move it. “It was just mind-boggling for an eight-year-old kid. It was like literally holding magic in your hands,” Elshenawy said. The device did more than teach Elshenawy chess; it inspired a lifelong passion for technology that can think and engage at a human scale. Today, Elshenawy works with cutting-edge artificial intelligence and machine learning to teach cars to drive as the EVP of Engineering at Cruise, a leading developer of autonomous vehicle technology. Riviera Partners checked in with Elshenawy for his insight into the key factors he considers crucial as a prominent leader in the tech industry.

---

# Running Car Simulations on GCP. I watched some of Cruise’s videos on… | by Rohit Jhangiani | Medium

## Unknown date - rohitjhangiani.medium.com

Source: https://rohitjhangiani.medium.com/running-car-simulations-on-gcp-154b69c4432c

Running Car Simulations on GCP I watched some of Cruise’s videos on Youtube and jotted down some notes on Cruise’s simuation infra built on GCP. For people getting started and/or interviewing in the AV space, here is a summary and quick read: - Performance testing — car stack against real car compute hardware to benchmark — latency, CPU, memory, GPU use - 100–1000s variations for each scenarios - Each test requires large runtime assets - Over 100 GB of data — copied onto the worker (node) - Binary assets, map tiles, ML models, recorded sensor data - Tests produce detailed matrix and data - Much more than a pass/fail result e.g. like driving well on the road - Pair simulation with on-road driving to find new and unique situations on the road Testing infra at Cruise - 200,000+ hours/day of simulations - 30000+ VM instances - 300,000+ CPU cores - 5000+ GPUs - 3 regions, 10 zones - 300 TB/day of results - 180 cars registered in CA as of 2019/2020 - 100x more times cars in simulation - 1/3 PB day of results from all these simulations Scale Challenges - Rapid scheduling - CPU and GPU resource availability - Efficient utilization of VM instances - Distributing job inputs to workers - Data analytics - Custom machine worker types - Latest gen GPUs and hardware - Spanner, Pub/Sub, BigQuery, Google Cloud Storage Hydra — job scheduler for code integration + running tests - CPU Docker - GPU/Nvidia Docker (to provide access to the hardware) - PostgreSQL - Hydra v2 — written in Go - Scheduling DB - Uses BigQuery — much more scalable datawarehouse Google Compute Engine - Managed instance groups — similar to autoscaling groups in AWS - Can Scale each zone - 1000 instances per group — limitation - Sharded instance groups - Worker Pools - Multi-regional buckets e.g. select a continent e.g. US - Data replicate across regions in the US- - Terraform modules - Templates for what an instance group should look like - Templates for what a zone should look like - Templates for what a region should look like Scaling is about 2 things: - Queue times - Utilization Autoscaling at Cruise - Queues with priorities - Avoid interrupting long running tasks - GCP Autoscaler — target CPU use or queue length - Default autoscaler takes down a random instance - Cruise built their own autoscaler Google cloud storage - Application binary, map tiles, ML models, sensor data - Write throughput = 1 GB/second - Read throughput = 380 GB / second Hydra improvements - Handle increasing load - Task pipelining (pre-fetch tasks for the next job) - Resource based optimization — send jobs where data is already cached Data analysis Old Analysis Architecture - Avro tables stored in S3 - Jupyter notebooks to read from Avro/S3 and create dashboards New Architecture and ETL Pipeline - 10TB+/day Custom front-end analysis tool - Calculate selected gap length when making an unprotected left turn (UPL) — baseline, feature and diff

---

# AV Sensors + Algorithms + Data Requirements - Rohit Jhangiani - Medium

## Unknown date - rohitjhangiani.medium.com

Source: https://rohitjhangiani.medium.com/av-sensors-algorithms-data-requirements-d5f699bdbdf5

AV Sensors + Algorithms + Data Requirements Mar 4, 2023 I read some online articles + watched some of Cruise’s videos on Youtube and jotted down some notes on how Autonomous Vehicles work. For people getting started and/or interviewing in the AV space, here is a summary and quick read: LiDAR and Radar frequencies - Radar wavelength = 30 cm to 3 mm - Radar frequency = 400 MHz to 36GHz - Lidar wavelength = 905 nm to 1550 nm - Lidar frequency = 190 THz to 330 THz Algorithms used in Self Driving Cars - Bayesian Regression - Neural Network Regression - Decision Forest Regression Activation Function - Softmax — used to get multinomial probability distribution Data Requirements (my rough calculations)

---

# Cruise Under the Hood Overview. I watched Cruise’s Under the hood event… | by Rohit Jhangiani | Medium

## Unknown date - rohitjhangiani.medium.com

Source: https://rohitjhangiani.medium.com/cruise-under-the-hood-overview-d3c94d66bfab

Cruise Under the Hood Overview I watched Cruise’s Under the hood event on Youtube and jotted down some notes on how Cruise’s Under the hood event. For people getting started and/or interviewing in the AV space, here is a summary and quick read: Sensors Multi-sensor input - Camera - LiDAR - Audio - Radar — for low visibility (to see through) - Need to handle all these long tail cases Multi-task output - Object tracking — measure velocity and turn rate - Traditional methods — Kamlan filter or particle filter - Deep learning is much better than the above filters - E.g. Deep learning pick up sudden pull out of parking 0.7 seconds earlier Challenges - #1 — Decision Density - #2 — Uncertainty Cruise Decision Engine Simulation System - Morpheus - Road to Sim - Based on NPC (non-player character) AI - World gen — system that can procedurally generate entire cities - Creates maptiles in the city - We can also add custom content using World gen Accelerating the AV development cycle (Code to Road) - Automatic event extraction - Automatic event classification - Webviz — change, run, analyze cycle takes a few seconds - Inspector — can applied to radar data - Inspector — model evaluation, A/B comparison, drill down to specific samples, multiple modalities etc. - AI devs train and export 100s of models every week - Inspector can analyze — past, present and future to understand the model performance - Starfleet — fleet management tool Cartographer — mapping system - Self updating and self healing map - Cruise has end to end system tools + infra - Simulating 17 years worth of driving data every day - Extra data from a data lake that contains exabytes of data + events - 1% of data contains valuable info, 99% noise - Most advanced AV tech + most advanced ecosystem Cruise Origin - For ride hailing and delivery fleet - Telematics in a car — vehicle onboard communication services and applications that communicate via GPS receivers and other telematics devices - Data processed — 10 GB per second

---

# How do Autonomous Vehicles work?. I read some online articles + watched… | by Rohit Jhangiani | Medium

## Unknown date - rohitjhangiani.medium.com

Source: https://rohitjhangiani.medium.com/how-do-autonomous-vehicles-work-c80807ccd5d5

How do Autonomous Vehicles work? I read some online articles + watched some of Cruise’s videos on Youtube and jotted down some notes on how Autonomous Vehicles work. For people getting started and/or interviewing in the AV space, here is a summary and quick read: Levels of self driving - Level 0 — on the roads today - Level 1,2,3 — a system driving the car - Level 2 or 3 — Tesla autopilot or Cadillac / GM super cruise - Level 4 — the car is driving itself | backup system in case of safety - Level 5 — car is driving by itself | not constrained based on weather and geography 3 main parts of the system — Vehicle, sensors, software stack AV stack - Sensors + sensor fusion - Localization - Mapping - Perception - Prediction - Planning - Control Sensors - LiDAR (how far are objects/agents) - Camera (what) - Radar - Ultrasonics - Thermal - Mic - GPS - Inertia sensors Software - Receive data — object detection - Perception stack — what is out there, what is it going to do next (prediction) Mapping - Identify objects - Identify routes LiDAR challenges - Weather - Being able to see in degraded environments of fog or rain Cameras - Most important sensor - All the imagery is fed to ML models - Classify when you see a person, a dog etc. Radar - Great for cutting through weather - How fast objects are going? - Does not have as much precision as camera and LiDAR - E.g. if you see something big rolling down the road, you don’t know if it is a semi vs a car etc. Ultrasonics - Very slow speeds - Park and see any shorter obstacles or people around Thermal cameras - To see better at night Microphone - Capture sounds e.g. police car or ambulance - Listen to instructions Software Stack - Receive data: Receive that sensor data and process the data - Perception Stack: That data is fed into perception - Once we get the profile of what the object is e.g. person or a dog, then we predict their behaviors - Person and dog might have different behaviors - E.g. person might be waiting at the corner - E.g. dog might run across the street - Mapping - Mapping car is used to identify objects - Mapping car is sent in advance so the AV can check against this map - Mapping car goes down the street and identifies sign posts, banks, still locations etc. - That way, when the autonomous car goes through, it can know where it is at any given time - Mapping is required in these test vehicles - GPS and inertia sensors are also useful for the car to know where it is at any given point of time - Perception - What is out there - Where are you going to go next - Factors: Your own motion planning - E.g. if person standing on the right hand side and you have to make a left turn - You can safely make a left turn Motion Planning - Once you have your motion planning - You are connected to the car’s controller in/via/and a system called CanBus - That’s how we give instructions to the car about - Where to move - Apply the gas or brakes - Talking machine (AD-ECUs x 2) to machine (car computer controllers x 2) about where the car should move Goal: A system that is attentive and that can get us safely about where we need to go.

---

# Meet the 11 Power Players of the Self-Driving Industry - Business Insider

## Unknown date - www.businessinsider.com

Source: https://www.businessinsider.com/power-players-autonomous-vehicles-tesla-morgan-stanley-zoox-2021-4

Meet the 11 power players of the self-driving industry from leading companies like Tesla, Zoox, and Morgan Stanley - Insider asked self-driving startups, analysts, and VCs to nominate the industry's biggest stars. - The one rule: no CEOs. - The list features employees from Tesla, Zoox, Morgan Stanley, and more. While CEOs and founders get most of the attention in the startup world, even the best leaders need a strong team behind them. In the autonomous-vehicle industry, it can be difficult to identify the most important supporting players from the outside. Most companies are still testing and refining technology that could be years from widespread use, so it's not always clear whose products are working and whose aren't. To identify the unheralded stars, we surveyed analysts, venture capitalists, industry experts, and autonomous-vehicle companies to nominate essential employees who aren't CEOs. From a list of more than 40 people, Insider selected those who appear to be having the greatest influence on their companies and the industry as a whole. These are 11 of the autonomous-vehicle industry's power players. Lia Theodosiou-Pisanelli, head of partner product and programs, Aurora A former trade negotiator for the Obama administration, Lia Theodosiou-Pisanelli is now Aurora's head of partnerships, product, and programs. During her tenure, the Amazon-backed startup has signed deals with blue-chip companies like Toyota, Paccar, and Volvo. Theodosiou-Pisanelli and her team are responsible for defining the products Aurora builds for its growing list of partners and running the programs that deliver them. She's also helped coordinate the integration of Uber ATG's workforce into Aurora's following Aurora's 2020 acquisition of the unit. Before joining Aurora, in 2019, she was Lyft's director of business development and strategy, where she focused on automated-driving technology. Andrej Karpathy, senior director of AI, Tesla Andrej Karpathy is a key figure behind the development of Tesla's Autopilot driver-assistance system. As Tesla's senior director of artificial intelligence, Karpathy oversees Autopilot's neural networks, which allow the system to learn to make better decisions by studying videos and images captured by Tesla vehicles. Though some have criticized Tesla for releasing new Autopilot capabilities in an unfinished form and for declining to develop a more robust system to make sure drivers pay attention to the road, Autopilot has broken ground for driver-assistance systems and received high marks for its capabilities. Before joining Tesla, in 2017, he worked at OpenAI as a research scientist. Puneeth Meruva, associate, Trucks Trucks is one of the top venture-capital firms in the autonomous-vehicle industry. According to PitchBook, it's tied with three VC firms for the highest number of investments in autonomous-vehicle companies with 18. Puneeth Meruva is the firm's "first filter" in deciding which companies to invest in, Trucks general partner Reilly Brennan told Insider. The firm looks at about 100 startups a month, and Meruva is usually the first person to meet with them. "He's probably looked at more AV companies than anybody anywhere," Brennan said. Meruva was a software engineer at Uber and BMW before joining Trucks. Regina Savage, global head of automotive and mobility technology, Morgan Stanley The autonomous-vehicle industry has been a hot sector for investment banks in recent years, and Regina Savage has emerged as one of the industry's key dealmakers, Brennan said. She's helped put together funding rounds and acquisitions for companies like TuSimple and NuTonomy, the startup that turned into Motional. And, Brennan said, her connections are second to none. "Whenever I'm hearing about a big deal going down, I'm usually trying to get a hold of Regina to see what she's heard," he said. Before joining Morgan Stanley, Savage worked on mergers and acquisitions in the media industry at Goldman Sachs. Mohamed Elshenawy, senior vice president of engineering, Cruise Mohamed Elshenawy has led Cruise's software team through a period of rapid growth as the company works toward launching robotaxi services in San Francisco and Dubai. Elshenawy has helped Cruise engineers quickly test and deploy code while overseeing a cloud-computing infrastructure that held up well as the company shifted to a remote-work model in 2020, a Cruise representative said. During Elshenawy's tenure at Cruise, the company's approach has impressed experts, who have ranked Cruise among the most promising autonomous-vehicle companies. Before Cruise, Elshenawy led tech development for Amazon's ReCommerce service and warehouse offerings. David Estrada, chief legal and policy officer, Nuro David Estrada oversees Nuro's policy, legal, communications, and safety teams. During Estrada's tenure, Nuro became the first autonomous-vehicle company to receive an exemption from select federal safety standards. Since Nuro's delivery vehicles don't carry people, the National Highway Traffic Safety Administration ruled they didn't need some features that would be required in passenger cars, like side mirrors and windshields. Under Estrada, Nuro has also become the first company to test driverless vehicles in three states, a Nuro spokesperson told Insider. Before joining Nuro, Estrada helped develop early autonomous-vehicle regulations at Google. Andy Piper, vice president of vehicle development, Zoox Zoox has taken a difficult and unusual approach compared to its peers: While most of its rivals are focusing on automated-driving software or hardware, Zoox is designing and assembling an autonomous vehicle, all without the help of a major automaker. Andy Piper has led the development of Zoox's eponymous vehicle, which the company unveiled in 2020. It's the only custom-built robotaxi that can drive up to 75 mph, a Zoox representative said. If Zoox's all-in-one strategy pays off, Piper will have played a major role in making it work. Piper is an auto-industry veteran with a résumé that included Saab, Ferrari, and Qoros Auto before he landed at Zoox in 2016. Laura Major, CTO, Motional Motional is one of two companies that operate autonomous ride-hailing vehicles that are open to the public. As Motional's CTO, Laura Major oversees the technology behind those vehicles. In recent months, Motional has taken the backup drivers out of some of its test vehicles and, in 2023, the company plans to expand its relationship with Lyft by launching a fully autonomous ride-hailing service. Motional expects to start adding robotaxis to Via's network during the first half of this year. "Laura and the Motional team are behind some of the self-driving industry's largest leaps forward," a Motional spokesperson told Insider. Gaby Hayon, executive vice president of research and development, Mobileye Mobileye has been a leader in bringing automated-driving technology to the road through its advanced driver-assistance software and hardware. Hayon, one of Mobileye's first employees, led the development of software that allows Mobileye ADAS systems to understand how nearby objects are moving so that they can warn the driver or take evasive action to avoid a collision. The software has helped drive the introduction or improvement of numerous safety and convenience features, including automatic emergency braking and steering assistance, a Mobileye spokesperson said. Hayon is leading the company's work on self-driving technology with an approach the company calls "true redundancy." It involves creating two autonomous systems — one using only cameras, the other radar and lidar — that can operate safely on their own, then bringing the two systems together. "In True Redundancy, Hayon has designed for future AVs a system that ensures uncompromised safety and unmatched scalability," the spokesperson said. Ashu Rege, senior vice president of software engineering, Zoox An early member of Zoox, Ashu Rege leads the company's software team. His contributions have been essential in helping Zoox develop its automated-driving system, Shahin Farshchi, a partner at the venture capital firm Lux Capital, a former Zoox investor, told Insider. And, Farshchi added, Zoox's software has played a key role in setting the company apart from its competitors. "I would credit a lot of what you find that's differentiating to him and his team," Farshchi said of Rege. Before joining Zoox, Rege was an executive at Nvidia, where he oversaw the development of the company's automated-driving platform and drone technology. Brett Browning, CTO, Argo AI Brett Browning leads the development of Argo AI's automated-driving technology. That technology has attracted investments from Ford and Volkswagen, which have put more than $3 billion into the Pittsburgh startup. In 2022, Ford plans to launch a delivery service with self-driving vehicles powered by Argo's tech. Browning and Argo have also impressed Sam Abuelsamid, a research analyst at Guidehouse Insights who covers automotive technology. "They've made some really good progress," said Abuelsamid, who speaks with Argo regularly. Browning was a senior faculty member of Carnegie Mellon's Robotics Institute and worked on mapping and localization technology for Uber's autonomous-vehicle unit before joining Argo, in 2016.

---

# 9 Top Cruise Engineers Bringing Driverless Rides to the Masses - Business Insider

## Unknown date - www.businessinsider.com

Source: https://www.businessinsider.com/top-cruise-engineers-bringing-driverless-rides-to-the-masses-2022-5#mo-elshenawy-executive-vice-president-of-engineering-1

Cruise is going to start charging for rides. Meet the 9 engineers from GM's self-driving bringing the first robotaxi service to the masses. - Cruise's engineering team survived a time of uncertainty with a "resilience mindset." - The self-driving startup hopes to pounce on an $8 trillion market for AVs. - Cruise launched the first fully driverless ride in San Francisco earlier this year. Cruise, the self-driving startup owned by General Motors, will be the first driverless taxi provider to charge a rare for its rides, propelling forward its plans to offer driverless taxi rides to the masses while many of its competitors have either moved away from driverless cars or pivoted to more commercial applications like trucking. At the start of this year, Cruise began piloting fully driverless taxis — without a safety driver behind the wheel — in a major urban environment for members of the public. After about four months of a nighttime pilot program on the streets of San Francisco, Cruise in June won approval from the California Public Utilities Commission to charge for its driverless rides. The company is offering driverless rides in western San Francisco through the app its product-engineering team built. Cruise is currently using its Chevrolet Bolt-based autonomous vehicles; the Cruise Origin, the startup's first AV built from the ground up, remains in development. The team behind this achievement has been working quietly for the past several years as investors and other engineers lost hope in self-driving technology. A pedestrian death in 2018 caused by an Uber vehicle in self-driving mode spooked many investors and brought a new level of scrutiny to the space. In this period, many startups ran out of money and hit the wall on technological development, thinning a herd of self-driving players that once powered the automotive industry's hype machine. With competition cut down, Cruise is hoping it can pounce on what its leaders say is an $8 trillion market waiting in the robotaxi and driverless-delivery space. "One of the biggest shifts for us in the last few years is to have this resilience mindset," Mo Elshenawy, Cruise's executive vice president of engineering, said. Elshenawy said it was important to be honest with his team about challenges and roadblocks while creating an environment where engineers can "embrace failure." He wanted his engineers to take risks without losing sight of the larger mission to safely put driverless cars on the road. The quality of talent that startups attract is a major contributing factor in leading the autonomous-vehicle space, according to automotive-industry experts. In the past several years, Cruise has grown its team from around 40 employees, when GM acquired it in 2016, to 1,800 at the start of this year. Here are the nine power players at Cruise leading the startup's charge to bring driverless rides to the masses. Mo Elshenawy, executive vice president of engineering Elshenawy heads the biggest team at Cruise and oversees the work of some 1,700 engineers tasked with developing every aspect of self-driving tech. The expertise on his team ranges from AI, robotics, and product engineering to mapping, safety, and hardware. This diverse team of engineers recently celebrated the launch of Cruise's pilot driverless-ride service in San Francisco, a major win for a group that had worked tirelessly for years, Elshenawy said. "There's only so much inspiration you can pump into your team yourself," Elshenawy said. "It's very different when you can get into this car at the end of the day and see all of your hard work." Prior to Cruise, Elshenawy led global technologies for Amazon's ReCommerce platform, warehouse deals, and liquidations. He has also served as cofounder and CTO for three tech startups, including a cloud-based financial-services-development platform that top financial institutions use. Tim Piastrelli, vice president of security A strong web of product security, information technology, and threat detection and response are the keys to Cruise's plans to manufacture a purpose-built autonomous vehicle. Piastrelli and his team are constantly securing Cruise's technology via a battery of tests and simulations designed to identify potential attacks and sources of vulnerability. With 15 years of experience in the field, Piastrelli said he was able to assemble a dream team of sorts for Cruise's security division that includes the hackers who remotely hijacked a Jeep Cherokee in a 2015 viral video for Wired. "My primary focus initially was to pull in a few of the rock stars from my network," Piastrelli said. "As I built my specific threat models, I was able to articulate to leadership what specific people we needed to hire." Before joining Cruise, Piastrelli headed security research at the software startup Mulesoft, where he also helped it complete its IPO. Cristina Scheau, director of perception Cruise's perception team builds the technology that "sees" the road and other obstacles around a self-driving car. As leader of the perception team, Scheau has overseen millions of miles in real-world and simulation testing to improve the way Cruise's vehicles navigate the streets of San Francisco. Scheau joined Cruise two years ago, and since then she has spent much of her time recruiting and retaining talent in the areas of applied science, research, and engineering. Before joining Cruise, Scheau was the director of engineering at Facebook. Wei Luo, vice president of strategy, product, and operations Luo is an executive focused on the big picture for Cruise as it moves closer to commercialization. Some of her work includes collaborating with leadership to set clear long-term goals for scaling and expanding the business while helping teams across the company work efficiently to meet those marks. Luo oversees several departments within Cruise, including the teams tasked with some of the company's most critical programs: ride-hailing, delivery, and vehicle-build programs. Prior to Cruise, Luo served as COO of DeepMap, a software company, where she worked on the self-driving-mapping company's go-to market strategy. Bhavini Soneji, vice president of product engineering Soneji oversees work on Cruise's two main products: the ride-hailing app and the driverless fleet. She drives the strategy and execution for these two customer-facing products while also leading a team that is building the mapping platforms to guide the robotaxis on the road. Prior to Cruise, Soneji was the vice president of engineering and technology at Headspace, a therapy app, and led teams at Heal, Snapchat, and Microsoft. Hussein Mehanna, vice president of AI and machine learning Cruise has spent years amassing millions of miles of on-road and simulated testing for its driverless cars. Mehanna leads the team that leverages the datasets from those tests to build artificial-intelligence models that can teach Cruise's fleet how to drive more like a human on unpredictable urban roads. Prior to joining Cruise in 2019, Mehanna worked as an engineer with some of the biggest names in tech, including Google, Facebook, and Microsoft. Vinoj Kumar, vice president of infrastructure Kumar leads the team working on the infrastructure that underpins Cruise's entire fleet of autonomous vehicles. Kumar and his engineers have taken a "cloud-native" approach to Cruise's tech infrastructure, which means that the software has been designed for use in the cloud from the very beginning of development. Kumar spent a decade at Google working in technical infrastructure before joining Cruise last year. Rob Flenniken, senior director of engineering and embedded systems Flenniken is one of the newest members of the Cruise engineering team, joining last month to lead the embedded-software engineering work the Cruise robotics team performs. Flenniken oversees a team working closely with hardware engineers to design and build the sensors, networks, and computer platforms that power the Cruise fleet. Flenniken came to Cruise in April 2022 with more than 20 years of experience in the consumer-electronics industry, most recently at Amazon, where he led device-software teams to build the first Alexa device, the Echo. Louise Zhang, vice president of safety and systems With Cruise beginning to offer rides to the masses, Zhang has the critical task of overseeing safety and validation of the startup's driverless fleet. She has an instrumental role in every phase of development for Cruise's vehicles from design to testing to commercial deployment. Zhang's team is working on both future and current Cruise vehicles, validating autonomous-driving systems and developing operations-safety guardrails for deployment. Zhang came to Cruise in 2020 with a strong background in vehicle safety. She was previously the senior director of engineering at Lucid Motors, an EV startup, where she oversaw vehicle safety and regulatory compliance. Prior to that, Zhang was head of Tesla's safety team.

---

# How Drive.ai Is Mastering Autonomous Driving With Deep Learning - IEEE Spectrum

## Unknown date - spectrum.ieee.org

Source: https://spectrum.ieee.org/how-driveai-is-mastering-autonomous-driving-with-deep-learning

Among all of the self-driving startups working toward Level 4 autonomy (a self-driving system that doesn’t require human intervention in most scenarios), Mountain View, Calif.-based Drive.ai’s scalable deep-learning approach and aggressive pace make it unique. Drive sees deep learning as the only viable way to make a truly useful autonomous car in the near term, says Sameep Tandon, cofounder and CEO. “If you look at the long-term possibilities of these algorithms and how people are going to build [self-driving cars] in the future, having a learning system just makes the most sense. There’s so much complication in driving, there are so many things that are nuanced and hard, that if you have to do this in ways that aren’t learned, then you’re never going to get these cars out there.” It’s only been about a year since Drive went public, but already, the company has a fleet of four vehicles navigating (mostly) autonomously around the San Francisco Bay Area—even in situations (such as darkness, rain, or hail) that are notoriously difficult for self-driving cars. Last month, we went out to California to take a ride in one of Drive’s cars, and to find out how it’s using deep learning to master autonomous driving. As its name suggests, Drive.ai was born for this. It was founded in 2015 by deep-learning experts from Stanford University’s Artificial Intelligence Laboratory. By structuring its approach to autonomous driving entirely around deep learning from the very beginning, Drive has been able to rapidly and adaptably scale to safely handle the myriad driving situations that autonomous cars need to master. “I think this is the first time autonomous driving has been approached so strongly from a deep-learning perspective,” says Tandon. “This is in contrast to a traditional robotics approach,” continues Carol Reiley, cofounder and president. “A lot of companies are just using deep learning for this component or that component, while we view it more holistically,” says Reiley. The most common implementation of the piecemeal approach to which they’re referring is the use of deep learning solely for perception. This form of artificial intelligence is good for, say, recognizing pedestrians in a camera image because it excels at classifying things within an arbitrary scene. What’s more, it can, after having learned to recognize a particular pattern, extend that capability to objects that it hasn’t actually seen before. In other words, you don’t have to train it on every single pedestrian that could possibly exist for it to be able to identify a kind old lady with a walker and a kid wearing a baseball cap as part of the same group of objects. While a pedestrian in a camera image is a perceptual pattern, there are also patterns in decision making and motion planning—the right behavior at a four way stop, or when turning right on red, to name two examples—to which deep learning can be applied. But that’s where most self-driving carmakers draw the line. Why? These are the kind of variable, situation-dependent decisions that deep-learning algorithms are better suited to making than the traditional, rules-based approach with which they feel more comfortable, Reiley and Tandon tell us. Though deep learning’s “humanlike” pattern recognition leads to a more nuanced behavior than you can expect from a rules-based system, sometimes this can get you into trouble. The Black Box A deep-learning system’s ability to recognize patterns is a powerful tool, but because this pattern recognition occurs as part of algorithms running on neural networks, a major concern is that the system is a “black box.” Once the system is trained, data can be fed to it and a useful interpretation of those data will come out. But the actual decision-making process that goes on between the input and output stages is not necessarily something that a human can intuitively understand. This is why many companies working on vehicle autonomy are more comfortable with using traditional robotics approaches for decision making, and restrict deep learning to perception. They reason that if your system makes an incorrect decision, you’d want to be able to figure out exactly what happened and then make sure that the mistake won’t be repeated. “This is a big problem,” Tandon acknowledges. “What we want to be able to do is to train deep-learning systems to help us with the perception and the decision making but also incorporate some rules and some human knowledge to make sure that it’s safe.” While a fully realized deep-learning system would use a massive black box to ingest raw sensor data and translate that into, say, a turn of the steering wheel or activation of the accelerator or brakes, Drive has intentionally avoided implementing a complete end-to-end system like that, Tandon says. “If you break it down into parts where you’re using deep learning, and you understand that different parts can be validated in different ways, then you can be a lot more confident in how the system will behave.” There are a few tricks that can be used to peek into the black box a little bit, and then validate (or adjust) what goes on inside of it, say the Drive researchers. For example, you can feed very specific data in, like a camera image with almost everything blacked out except the thing you want to query, and then see how your algorithm reacts to slightly different variations of that thing. Simulation can also be a very helpful tool when dealing with specific situations that an algorithm is having difficulty with, as Tandon explains: When we first started working on deep-learning perception systems, one of the challenges we had was with overpasses. We’d go out and drive, and the shadow caused by an overpass would cause the system to register a false positive of an obstacle. In the learning process, you can focus the algorithm on challenging scenarios in a process called hard mining. We then augment the data set with synthetic examples, and with that, what you do is say, “Hey, system, tell me what you’re going to do on this overpass, and then I’m going to jitter it a little bit, and you’re going to do it again.” Over time, the system starts to work around overpasses, and then you can validate it on a systematic level. Training the System Deep-learning systems thrive on data. The more data an algorithm sees, the better it’ll be able to recognize, and generalize about, the patterns it needs to understand in order to drive safely. For autonomous cars, which need to be able to understand a vast array of different situations, the default approach taken by most companies working on autonomous driving is to just collect as much data as possible. The issue, then, becomes managing the data and then doing something useful with it. Drive.ai keeps in mind that data is not all created equal. The company puts an immense amount of effort into collecting high quality data and then annotating it so that it’s useful for training deep-learning algorithms. Before cars can drive themselves, the laborious task of annotating the objects in every scene a self-driving car’s sensors capture must be completed. This data is what feeds deep-learning or rules-based algorithms.Image: Drive.ai Annotation, while very simple, is also very tedious: A human annotator is presented with a data set, perhaps a short video clip or even just a few frames of video or lidar data, and tasked with drawing and labeling boxes around every car, pedestrian, road sign, traffic light, or anything else that might possibly be relevant to an autonomous driving algorithm. “We’ve learned that certain companies have a large army of people annotating,” says Reiley.”Thousands of people labeling boxes around things. For every one hour driven, it’s approximately 800 human hours to label. These teams will all struggle. We’re already magnitudes faster, and we’re constantly optimizing.” How is that possible? Drive has figured out how to use deep-learning-enhanced automation for annotating data. So, it has a small band of human annotators, most of whom are kept busy training brand new scenarios or validating the annotation that the system does on its own. “There are some scenarios where our deep-learning system is working very well, and we just need to have a validation step,” Tandon explains. “And there are some scenarios where we’ve improving the algorithm and we need to bootstrap it the right way, so we have a team of human annotators do the first iteration, and we iteratively improve the deep-learning system. Already in many cases, our deep-learning systems perform better than our expert annotators.” Reiley adds: “Think about how mind-blowing that is.” It’s difficult for the Drive team to articulate exactly what prevents other companies from building their own deep-learning infrastructure and tools and doing the same kind of deep-learning-based annotation and training. “We talk about this quite often: What’s to stop someone else from doing exactly what we’re doing?” says Tandon. “To be honest, there are just so many parts to the problem. It’s such an integrated system; there’s just so many components to get right throughout the entire stack, that it becomes hard to say there’s one specific reason why this works well.” Reiley agrees: “Your decisions have to be software driven and optimized for deep learning, for software and hardware integration. Everyone focuses only on the algorithm portion of it, but we have these other applications that all need to come together. Autonomous driving is much more than just an algorithm— it’s a very complicated hardware-software problem that nobody has solved before.” Sensors in the Rain The hardware in Drive’s four-car fleet is designed to be retrofit onto most vehicles with a minimum of hassle and is concentrated in an array of sensors, including cameras and lidar, located on the roof. The system also takes advantage of the car’s own integrated sensors, like radar (used for adaptive cruise control) and rear cameras. There’s also a big display that Drive eventually plans to use for communicating with human drivers and pedestrians; we go into that in more detail in this article. With a suite of nine HD cameras, two radars, and six Velodyne Puck lidar sensors, each of Drive’s vehicles is continuously capturing data for map generation, for feeding into deep-learning algorithms, and of course for the driving task itself. The current sensor loud out is complex and expensive, but as Drive cofounder Joel Pazhayampallil explains, it’s almost certainly overkill, and will be reduced when Drive moves into pilot programs. “I think we’ll need a significantly smaller subset, probably half what we have right now, if that,” Pazhayampallil says. “Our algorithms are constantly improving. We’re constantly getting more and more out of each individual sensor by combining data from the different sensors together. We get some low-resolution depth data from the lidar and really high-resolution context information from the camera.” This kind of multimodal redundancy and decision making through deep learning based on fused sensor data has advantages in an autonomous vehicle context. Namely, it offers some protection against sensor failure, since the deep-learning algorithms can be trained explicitly on perception data with missing sensor modalities. Deep learning has a significant advantage over rules-based approaches here, since rules conflicts can lead to failures that can be, according to Pazhayampallil, “catastrophic.” And sensor failure is most often not a hardware or software issue but rather a sensor that isn’t producing good data for some reason, like sun glare, darkness at night, or (more commonly) being occluded by water. The reason that driving in the rain is a challenge for autonomous cars isn’t just that water absorbs the lidar energy, or that surfaces turn reflective. You can’t really tell in the above video, but Drive showed us feeds from the car’s roof cameras, which had big drops of water on the lenses that rendered them mostly useless. “If you’re driving in harder, more nuanced situations, you need to be able to handle camera failures, lidar failures, radar failures, whatever happens in the real world,” Tandon says. On the Road We took our demo ride with Tory Smith, Drive’s technical program manager. Sadly, we managed to miss all of the horrible weather that California has been having over the past month, and went out on the sort of dry and sunny (but not too sunny) day that autonomous cars love. Drive is targeting Level 4 autonomy. But, in accordance with current laws, the company is testing its vehicles under Level 2 autonomy. At Level 2, a human driver must be in the driver’s seat, ready to take over at any time, although the car is expected to do most (if not all) of the driving by itself. While Smith and I talked about what the car was doing, Leilani Abenojar, one of Drive.ai’s autonomous-vehicle operators, stayed focused on the road. The 20-minute ride through a premapped area of suburban Mountain View, Calif., featured 16 intersections and a four-way stop. In general, the car performed smoothly and competently, but it was less assertive than an average human driver would be. This is intentional, says Smith: “We have to design for an acceptable envelope of performance, and it’s very difficult to have a vehicle that’s assertive but [doesn’t cross the line into being so] assertive that our guests or drivers are uncomfortable. We always operate with an abundance of caution, and would always rather that you be bored than be uncomfortable.” For our demo ride, that aim was achieved. Realistically, a boring experience is exactly what you want from a safe and trustworthy autonomous car. There was one exception, where our safety driver had to disengage the autonomous system and take manual control. And although we were never at risk of any sort of accident, it’s these exceptions that provide the best perspective on the current state of Drive’s autonomy. We were stopped at a red light, waiting to make a right turn onto a main road. Right turns on red are legal in California, but Drive doesn’t usually try to make them autonomously because of sensor limitations, explains Smith. “Our lidar sensors can only see about 50 to 75 meters reliably, and on a road like this, where you can have cross traffic at 45 or 50 miles per hour [72 or 80 km/h], you can’t yet with sufficient confidence detect cross traffic and know exactly what lane it’s going to be in.” When the light turned green, the car made the turn into the rightmost lane (which is legally how a right turn is supposed to be made). But there was a truck pulled over on the side of the road, blocking that lane, so our safety driver briefly took over, steered around the truck, and then reengaged the autonomous system. “Nominally, we would have waited for the truck to get out of the way,” Smith told me. “In terms of path planning, having the vehicle compensate for obstructions on the fly like that is a place where we’re currently building in more capability.” This situation is more than just a path planning problem, though: Waiting for the truck to move is the right call, if the truck is active. If it’s not, you’d want to go around. Put yourself in the car’s position: How do you know if a stopped truck is likely to move again? Maybe you can tell whether the engine is running, or notice indicator lights flashing, or identify some activity around the truck. These are all things that a human can do almost immediately (and largely subconsciously), but an autonomous vehicle needs to be explicitly trained on what to look for and how to react. “Humans aren’t necessarily perfect at doing very precise things,” says Smith, “but they’re great at improvising and dealing with ambiguity, and that’s where the traditional robotics approach breaks down, is when you have ambiguous situations like the one we just saw. The nice thing about developing a system in a deep-learning framework is when you encounter difficult situations like that, we just have to collect the data, annotate the data, and then build that module into our deep-learning brain to allow the system to be able to compensate for that in the future. It’s a much more intuitive way to solve a problem like that than the rules-based approach, where you’d basically have to anticipate everything that could ever happen.” Disengagements like these are where most of Drive’s most valuable learning happens. It’s actively looking for test routes that include challenges, leading the company to do a significant amount of testing in the San Francisco Bay Area, which can be stressful even for human drivers. Drive’s modus operandi: Once its vehicles can reliably navigate a route without getting disengagements, the team will pick a new route with different challenges. Because it can annotate and train on new data so efficiently, Drive expects that its speed of adaptability will enable it to conquer new terrain very rapidly. Humanlike Driving At the end of our demo drive, Smith asked me if I thought the car’s driving was more “humanlike” than other autonomous car demos I’d had in the past. Describing any robot as doing anything “humanlike” can get you into all sorts of trouble, so I asked Smith to explain what it means, from a technical standpoint, for an autonomous car to exhibit humanlike behavior or intelligence. Smith gave me an example of how Drive does traffic light detection. The prevailing methodology for detecting traffic lights is to map out every intersection that your car will drive through in enough detail that you can tell your cameras exactly where to look. For a closed course, you can use this kind of brute-force approach, but it breaks down if you try to scale it up even to the level of a city. Drive instead has collected more generalized data on traffic lights, annotating what a traffic light looks like in different intersections, from different angles, in daytime, in nighttime, and during rain and snow and fog. These annotated data are fed into Drive’s deep learning algorithms, and the system learns to recognize the general concept of a traffic light, much in the same way that humans do, as Smith explains: The other thing about deep learning that’s really nice is that we get to use the context of the entire scene, not just the lights themselves. As a human, for example, there are situations where you start to go at a green light without necessarily looking at the light itself, because you’re looking at what everyone else is doing. Our annotation tool and human annotators take that into account: Maybe they can’t see all the lights, but they see that the cars next to them are all moving, so it’s probably green. That system has actually accrued enough humanlike intelligence that our traffic light detector is now more accurate than a human, which is really exciting. And we can expand that same intelligence to other aspects of our deep learning brain. Out in the World For better or worse, Drive’s ability to maintain its aggressive, um, drive toward Level 4 autonomy (vehicles that operate in specific areas and conditions without a safety driver) likely depends, in the short term, on state and federal regulations. But Sameep Tandon, at least, is highly optimistic about the trajectory of Drive’s autonomous technology. “We’re in the process this year of doing pilots with some of our customers. In the next six months, we’re hoping to deploy this—not at a huge scale, but at least so that more people can use it. I believe that in geo-fenced areas, we could have a solution without a safety driver operating in the next year or two. Ultimately, the question will be how quickly can we go from the Bay Area to the next city and the one after that.” Drive’s plan is to focus initially on logistics: businesses that repeatedly deliver things across small areas, as opposed to something like ride sharing. This gives Drive a well constrained problem as well as a smooth expansion path, and avoids having to deal with human passengers, at least initially. Beyond that, Tandon is excited about the future: “I think if you have a combination of really good strategy and really good technology, this could be one of the first uses of robots in the real world. That, to me, is super exciting; I’d love to see robots everywhere, and self-driving cars are probably going to be the first that everyday people interact with.” Evan Ackerman is a senior editor at IEEE Spectrum. Since 2007, he has written over 6,000 articles on robotics and technology. He has a degree in Martian geology and is excellent at playing bagpipes.

---

# Driverless cars may already be safer than human drivers

## Unknown date - www.understandingai.org

Source: https://www.understandingai.org/p/driverless-cars-may-already-be-safer

Driverless cars may already be safer than human drivers I learned a lot reading dozens of Waymo and Cruise crash reports. I’m a journalist with a computer science master’s degree. In the past I’ve written for the Washington Post, Ars Technica, and other publications. Understanding AI explores how AI works and how it’s changing the world. If you like this article, please sign up to have future articles sent straight to your inbox. August was an eventful month for driverless taxis in San Francisco. On August 10, the California Public Utilities Commission voted to allow Google’s Waymo and GM’s Cruise to begin charging customers for driverless taxi rides across the city. A week later, Cruise vehicles were involved in two serious crashes within hours of one another. The next day, the California Department of Motor Vehicles demanded that Cruise cut its driverless taxi fleet in half while these crashes were investigated. A few days later, New York Times reporter Cade Metz appeared on the Times’s flagship podcast, The Daily, to discuss these developments and the state of the self-driving industry. Metz argued that in recent weeks, it has become “more and more clear to the people riding the cars, and to other citizens in the city, that they are flawed, that they do make mistakes, that they can gum up traffic, that they can cause accidents.” Of course self-driving cars are flawed—all technologies are. The important question is whether self-driving cars are safer than human-driven cars. And here Metz proclaimed ignorance. “We don't know yet whether it's safer than a human driver,” he said. But we actually do know a fair amount about the safety of driverless taxis. Waymo and Cruise have driven a combined total of 8 million driverless miles, including more than 4 million in San Francisco since the start of 2023.1 And because California law requires self-driving companies to report every significant crash, we know a lot about how they’ve performed. For this story, I read through every crash report Waymo and Cruise filed in California this year, as well as reports each company filed about the performance of their driverless vehicles (with no safety drivers) prior to 2023. In total, the two companies reported 102 crashes involving driverless vehicles. That may sound like a lot, but they happened over roughly 6 million miles of driving. That works out to one crash for every 60,000 miles, which is about five years of driving for a typical human motorist. These were overwhelmingly low-speed collisions that did not pose a serious safety risk. A large majority appeared to be the fault of the other driver. This was particularly true for Waymo, whose biggest driving errors included side-swiping an abandoned shopping cart and clipping a parked car’s bumper while pulling over to the curb. Cruise’s record is not impressive as Waymo’s, but there’s still reason to think its technology is on par with—and perhaps better than—a human driver. Human beings drive close to 100 million miles between fatal crashes, so it’s going to take hundreds of millions of driverless miles for 100 percent certainty on this question. But the evidence for better-than-human performance is starting to pile up, especially for Waymo. And so it’s important for policymakers to allow this experiment to continue. Because at scale, safer-than-human driving technology would save a lot of lives. Waymo’s impressive safety record Back in February, Waymo released a report celebrating its first million miles of fully driverless operation, which mostly occurred in the suburbs of Phoenix. Waymo’s autonomous vehicles (AVs) experienced 20 crashes during those first million miles. Here are some representative examples: “A passenger car backed out of a parking space and made contact with the Waymo AV.” “An SUV backed out of a driveway and made contact with the Waymo AV.” “The vehicle that had been previously stopped behind the Waymo proceeded forward, making contact with the rear bumper of the Waymo AV.” “A passenger car that had been stopped behind the Waymo AV passed the Waymo AV on the left. The passenger car’s rear passenger side door made contact with the driver side rear of the Waymo AV.” In short, these were mostly low-speed collisions initiated by the other diver. There were only two cases where a Waymo ran into another vehicle. In one, a motorcyclist in the next lane lost control and fell off their bike. The driverless Waymo slammed on its brakes but couldn’t avoid hitting the now-riderless motorcycle at 8 miles per hour. In the other case, another vehicle cut in front of the Waymo, and again the AV braked hard but couldn’t avoid a collision. There were two crashes that Waymo thought were serious enough for inclusion in a federal crash database. The more serious of these was when another driver rear-ended a Waymo while looking at their phone. One of Waymo’s biggest challenges during its first million miles was avoiding inanimate objects. Waymo vehicles bumped into a construction pylon, a parking lot barrier arm, and a shopping cart—all at speeds between 8 and 13 miles per hour. Clearly Waymo needs to do a better job recognizing irregularly-shaped objects like these. But when it comes to interacting with other vehicles, Waymo had a basically spotless driving record over those first million miles. Now let’s look at how Waymo has done in San Francisco since the start of 2023. Waymo is still struggling to avoid inanimate objects. Its vehicles collided with cardboard road debris and a chain connecting a sign to a temporary pole. A Waymo also drove into a pothole that was big enough to puncture a tire. And there were two incidents where Waymos scraped parked vehicles. That’s a total of five crashes where the Waymo vehicle was clearly at fault. The rest of Waymo’s driverless crashes in San Francisco during 2023 do not seem to have been Waymo’s fault. I count 11 low-speed crashes where another vehicle rear-ended a Waymo, backed into a stopped Waymo, or scraped a stopped Waymo while trying to squeeze by. There was also an incident where a Waymo got sideswiped by another vehicle changing lanes. Waymo had two more serious crashes in San Francisco this year: A driverless Waymo was trying to turn left but another car “proceeded into the intersection from the left and made contact with the left side of the Waymo AV.” An SUV rear-ended a Waymo hard enough that the passenger in the Waymo reported injuries. I should also mention the Waymo crash that killed a dog back in May. I didn’t mention this earlier because I’ve been focusing on driverless vehicles and the Waymo that hit the dog had a safety driver behind the wheel. But this crash is worth mentioning since it’s one of the most serious ones Waymo has experienced. In an emailed statement, Waymo said that it “reviewed the event from many different perspectives” and concluded there was no way either Waymo’s software or a human driver could have avoided hitting the dog. Waymo hasn’t provided the public with enough information to verify this claim, but I hope California regulators check Waymo’s work if they haven’t done so already. We don’t have great data on the safety of human drivers To sum up, Waymo’s driverless fleet has experienced: 17 low-speed collisions where another vehicle hit a stationary Waymo 9 collisions where another vehicle rear-ended a Waymo 2 collisions where a Waymo got sideswiped by another vehicle 2 collisions where a Waymo got cut off and wasn’t able to brake quickly enough 2 low-speed collisions with stationary vehicles 7 low-speed collisions with inanimate objects like shopping carts and potholes There are two things to notice about this list. First, other vehicles ran into Waymos 28 times, compared to just four times a Waymo ran into another vehicle (and Waymo says its vehicle got cut off in two of these cases). Second, Waymo was only involved in three or four “serious” crashes, and none of them appear to have been Waymo’s fault. This is impressive because these statistics reflect more than 2 million miles of driving.2 The National Highway Traffic Safety Board estimates that there are around 6 million car crashes reported to the police each year. Americans drive around 3 trillion miles per year, so roughly speaking a “major” crash occurs on the roads once every 500,000 miles. Most crashes involve two vehicles. So if Waymo’s vehicles drove as well as a typical human driver, you’d expect it to be involved in around eight serious crashes over 2 million miles of driving. It’s important to emphasize that there’s a lot of uncertainty about these figures. “We know very little about the safety of our roads,” the legal scholar Bryant Walker Smith told me. “If we're looking at just crashes, given how little information is carefully collected and studied, we don't have any sense of the circumstances of these low-level crashes.” Not all crashes—even serious ones—are reported to the police. Moreover, Smith said, “these companies are not driving a representative sample of miles.” Both Waymo and Cruise have their driverless cars avoid freeways, which tend to have fewer crashes per mile of driving. Both companies are active in San Francisco, which has more chaotic streets than most US cities. On the other hand, a small minority of drivers—including teenagers, elderly people, and drunk drivers—account for a disproportionate share of crashes. An alert and experienced driver gets into crashes at a rate well below the national average. So if we want AVs to drive as well as an alert and experienced driver, we might want to set the bar higher than the national average. With all that said, it seems like Waymo cars get into serious crashes at a significantly lower rate than human-driven cars. I’ll have more to say about this after we look at Cruise’s safety record. Cruise has room for improvement Cruise released a report back in April about its first million driverless miles. The company reported 36 crashes, compared to 20 for Waymo’s first million driverless miles. I wouldn’t put too much stock into that difference, since Cruise was operating mainly in San Francisco, a more chaotic driving environment than the Phoenix suburbs where Waymo started out. So far in 2023, Cruise has filed an additional 27 crash reports related to its fully driverless cars. What follows is a summary of all 63 crashes Cruise reported through August 25. I’ll also count a widely publicized August 17 crash with a fire truck even though there’s still no report on this crash on the website of the California Department of Motor Vehicles. Like Waymo, Cruise has had trouble with its vehicles hitting inanimate objects. Two Cruise vehicles ran into downed power cables. Cruise vehicles also ran into a scooter (without someone on it), a tow dolly on the back of a double-parked truck, a “motorized articulating boom lift,” and a pothole. The pothole punctured a tire, causing the Cruise AV to swerve into a parked car. Cruise has also experienced a large number of low-speed crashes where another vehicle (including a scooter in one case and a skateboarder in another) either rear-ended a Cruise AV, backed into one at low speeds, or scraped the side of a Cruise while trying to pass it. There were also a few only-in-San-Francisco situations: A Cruise vehicle was “stuck in a sideshow event and stationary with vehicles driving around it on either side.” (A sideshow is an illegal late-night show where young people perform donuts and other stunts in an intersection.) One of the other cars ran into the Cruise AV. An Infinity Q50 was “performing donuts” in an intersection before crashing into a Cruise vehicle. A driver drove the wrong way down a one-way street while staring at a phone. The car hit a stopped Cruise vehicle facing the right way. There were about a dozen side-swipe events where another vehicle either ran into the Cruise AV from the side during a lane change or tried to make a turn from a middle lane, crossing the path of the Cruise AV. Most of these crashes occurred during Cruise’s first million miles, so Cruise may be getting better at handling these situations. It’s important to note that Cruise has logged more than four million miles in San Francisco, so Cruise’s crash reports represent roughly twice as many miles as Waymo’s. Once you adjust for that, Waymo and Cruise seem to have been involved in low-stakes crashes at similar rates. For example, Cruise vehicles got rear-ended 17 times over about 4 million miles, while Waymo vehicles got rear-ended seven times over roughly 2 million miles. That makes sense given that Cruise drove twice as many miles and that Waymo logged almost half of its miles in the tame Phoenix suburbs. But even taking those differences into account, there are a couple areas where Cruise’s performance does not seem to be on par with Waymo. One is significant crashes where Cruise was clearly at fault. I saw three examples of this: A Cruise AV mistakenly thought the vehicle ahead of it was starting to turn left. The Cruise ran into the other vehicle when it turned right instead. A Cruise AV changed lanes when there wasn’t enough space to do so, cutting off another vehicle and leading to a crash. A Cruise AV ran into the back of a city bus. Cruise subsequently determined that its software got confused because it was an articulated bus (the kind with an accordion joint in the middle) and Cruise’s software couldn’t handle two parts of a vehicle moving in slightly different directions. Each of these mistakes strikes me as more serious than any of Waymo’s mistakes (recall that all of Waymo’s clearly at-fault crashes were low-speed collisions with inanimate objects or parked vehicles). Cruise might have a problem with intersections Cruise’s other trouble spot is intersections. Cruise says two bicyclists have run stop signs and crashed into Cruise vehicles. And there have been five vehicles that ran red lights and crashed into Cruise vehicles: This fire truck with its sirens blaring A passenger in that last Cruise AV was taken to the hospital; Cruise described their injuries as “non-severe.” Perhaps all of these crashes (with the possible exception of the fire truck) were the fault of the other drivers (and cyclists). Still, it’s interesting that over two million miles of driverless operation, no Waymo AVs got hit by cars running red lights or bicycles running stop signs. Again, this may be partly because Cruise has driven more miles—and especially more miles in San Francisco. Also, Cruise has largely operated at night when there might be more impaired drivers on the road. But I think there might be something else going on here. A couple of years ago, Waymo published research exploring the potential for self-driving cars to prevent crashes by anticipating the reckless behavior of other drivers. Waymo researchers obtained detailed records about fatal crashes that occurred in and around the Phoenix suburb of Chandler (where Waymo launched its first driverless taxi service). Waymo then hired an independent engineering firm to create detailed digital reconstructions of these crashes. Then the company loaded this data into its simulator to explore how Waymo’s self-driving software would have reacted in the seconds preceding each crash. Waymo found its software could prevent every crash if it took the role of the “initiator,” the vehicle whose erratic behavior set the crash in motion. More surprisingly, Waymo also found its software could prevent 82 percent of crashes playing the role of the other driver. The most common setting for fatal crashes in this data set was intersections—including a number of vehicles running red lights. Waymo found that when its software played the role of the “other driver,” it was able to avoid crashes in 81 percent of scenarios at intersections. In the wake of the Cruise collision with a fire truck on August 17, Waymo told industry analyst Brad Templeton that its vehicles would have handled the situation better than Cruise did: When we hear sirens, our vehicle will slow and then depending on how the situation develops, we will either pull over or stop ahead of intersections where there might be crossing emergency vehicles, even if we have a green light. The system is designed to remain cautious and not enter an intersection if it is still reasoning whether the emergency vehicle is approaching the intersection based on what it is sensing. I think technology like this may explain why Waymo has been successful at avoiding major crashes at intersections. Not only do Waymo’s vehicles follow the letter of the law (like stopping at red lights) they may also try to anticipate and avoid dangerous situations (like vehicles running red lights). Cruise vehicles do not seem especially cautious about intersections. For example, a Reddit user posted a video from August 22 showing a Cruise vehicle crossing an intersection several seconds after the opposing traffic got a green light. Cruise says its vehicle was already in the intersection when its light turned red so the vehicle didn’t break the law. Maybe that’s technically true—I’m not an expert on California traffic law. But I’m pretty sure it would have been safer for the car to stay where it was and wait for the next green light. Cruise’s technology is pretty good, but Waymo’s is better The bottom line is that I’m convinced that Waymo vehicles drive more safely than Cruise vehicles. This isn’t surprising; Waymo started its life as the Google self-driving project several years before Cruise was founded. Back in 2020, Waymo announced it had completed 20 million miles of on-road testing (almost all of them with safety drivers). The same year, Cruise reached 2 million miles. In short, Waymo has invested more time and resources into its technology. It would be surprising if all that extra work didn’t yield superior performance. With that said, I don’t want to be too negative about Cruise. Because while the company’s technology doesn’t seem to be as good as Waymo’s, it’s still pretty good. Earlier, I discussed why it’s so difficult to develop a good benchmark for human driving performance. We only know about crashes that get reported to the police or other authorities, giving us a patchy understanding of how many crashes really occur. Cruise tried to address this problem by hiring a team of prominent academic researchers to study the driving behavior of ride-hail drivers in San Francisco. The researchers examined 5.6 million miles of data and concluded that collisions involving San Francisco ride-hail drivers occur about once every 20,000 miles. That includes a lot of minor crashes that wouldn’t be reported to police. Based on this data, Cruise claimed that over its first million miles, its vehicles crashed 56 percent less often per mile than a typical human driver. Moreover, Cruise estimated that its cars were 73 percent less likely to be in a crash with a risk of a serious injury and 93 percent less likely to be the “primary contributor” to a crash. I’m inclined to take this with a grain of salt given that the research was commissioned by Cruise. But it doesn’t seem crazy. Cruise vehicles really do seem to crash into other vehicles much less often than vice versa. So I wouldn’t be surprised if Cruise vehicles already drive more safely than the average human driver. Don’t slow down progress The big question for policymakers is whether to allow Waymo and Cruise to continue and even expand their services. This should be an easy call with respect to Waymo, which seems to be safer than a human driver already. The faster Waymo scales up, the more crashes can be prevented. I think Cruise’s tech is probably safer than a human driver too, but it’s a closer call. I could imagine changing my mind in the coming months as more data comes in. Still, it’s important to remember that access to public roads is essential for testing and improving self-driving technology. This is not a technology Waymo or Cruise can meaningfully test “in the lab.” The companies need exposure to the full complexity of real public streets in order to make progress. And given that both companies are likely to eventually develop products that are much safer than human drivers, slowing down the development of the technology could easily cost more lives than it saves. So while the DMV’s decision to cut the size of Cruise’s fleet in the wake of the August 17 crashes was understandable, I hope the decision is short-lived. Ultimately the only way for Cruise to improve its technology is by testing it on public roads. And we’ll all benefit from the widespread availability of self-driving cars that are dramatically safer than human drivers. One easy way for policymakers to improve safety—or at least accountability—would to require self-driving companies to be more even more transparent about their safety records. This story relied heavily on California’s excellent website that publishes all of the Waymo and Cruise crash reports. I’d love for the California Department of Motor Vehicles to go a step further and require self-driving companies to submit video footage of the seconds before and after each crash. That way members of the public could evaluate whether companies’ descriptions of crashes are accurate. It would also be very helpful for regulators in other states—or perhaps federal officials—to require the same kind of crash reporting they have in California. For example, Waymo is running a substantial driverless taxi service in Phoenix, but we know very little about how well Waymo’s AVs have performed there in recent months. More transparency here and in other states could help to build public trust. Cruise announced on February 22 that it had driven 1 million miles and announced on August 11 that it had driven 4 million driverless miles. Most of those 3 million miles have happened in San Francisco. In an email, Waymo told me it has driven around 4 million driverless miles. A majority of those miles were in the Phoenix area, but at least a million have been in San Francisco since the start of 2023. Waymo’s February report covered 1 million miles up to January 2023. A Waymo spokeswoman told me the company has logged more than 1 million miles in San Francisco this year.

---

# End-to-end lateral planning - comma.ai blog

## Unknown date - blog.comma.ai

Source: https://blog.comma.ai/end-to-end-lateral-planning/

End-to-end lateral planning The goal of the research team at comma is to build a superhuman driving agent. We’ve discussed before how we believe the most effective way to build a superhuman system is to build an end-to-end planner that mimics human driving. So far openpilot has relied heavily on classical perception to detect lanelines and lead cars to make driving decisions. Eventually we want lateral and longitudinal planning to be done fully end-to-end, but our recent focus has been on lateral planning. The latest release of openpilot, 0.8.3, includes a new model that can do high quality lateral planning and was trained fully end-to-end. This post will give an overview of how these models work and the challenges involved with getting to this point. What is e2e and why is it hard? A simple approach We call a model e2e (end-to-end), when it takes in images/video and outputs how you should drive, in our case the model outputs a trajectory of where to drive in 3D space. This trajectory is represented by x, y and z coordinates in meters, and we leave it to a controller to execute this trajectory. The simplest approach one can imagine, is to collect a lot of data of humans driving, and then train a model that takes in video and predicts the trajectory the human drove for every situation. We use a type of MHP loss, to make sure the model can predict multiple possible trajectories. We start with a fully-conv efficientnet B2 block that reduces down to a 1024-dimensional vector, we call this block a vision model. This feature vector then goes into a block containing a few fully connected layers that we call a policy model. A simulator Testing whether things work in the real world is rather inefficient, so we use a simulator. We use a simulator that takes in real-world video and computes how openpilot would behave with a certain model. The model predicts where you should drive, the controls stack computes the ideal steering angle to achieve that trajectory and a simple physics model then calculates how the car would move with that steering angle. After one iteration of this loop, the input video now needs to be modified to reflect the movement of the car. This is the difficult part. One could run this in a simulation environment like CARLA, where the engine allows you to move the vehicle as you please and give you new camera perspectives. However, this would restrict the diversity of testing to what CARLA can simulate, we would be blind to a lot of potential real-world failures. Instead we opted to use real-world video and augment it to make it look as if the car is moving in the way we want it to. There are many ways to do this, but we just apply a simple warp to the image, assuming the whole scene is in the road plane. For any large movements this seriously distorts the images, but for small deviations this works well. The problem Now we have a simulator, we can test how the simple approach would perform. As you can see in the video below, it mostly correctly predicts where the human drove, however it still doesn’t stay on track well. And when we introduce some slight real-world noise (steering lag, wind, etc…) things get dramatically worse. What exactly is going here is worth a whole discussion in itself, but we’ll try to shed some light on it with a simple thought experiment. The simple models described above predict where a human is most likely to drive given some driving video. Imagine a long straight highway lane, where a human is perfectly centered in the lane. The human is most likely to keep driving completely straight, so that is what our model will predict. However, imagine the human was for any reason 10 cm away from the center, what is the human’s most likely trajectory now? It is still completely straight. If you accept that claim, you can quickly see how a model that just predicts a human’s most likely trajectory does not predict how to recover from mistakes. If the car deviates from the ideal trajectory for any reason, the model does not predict a trajectory that recovers back to the human trajectory. And so, when there is any noise that causes the car to deviate from the planned trajectory, there is no pressure to recover and the car keeps drifting away from the ideal trajectory. In practice there is always noise, even in a simulation where we don’t introduce noise, there are linearization errors, model prediction errors, rounding errors, etc… Our approach Learning in a simulator To solve the problem, we need to find a way to train a model such that it can predict how to recover from any deviations from a good trajectory. One way to do this is to use the simulator described earlier. We can introduce noise, so the car moves away from the ideal trajectory, and we can then train a model to predict where the human trajectory is from this new perspective. For our purposes we always consider the human trajectory to be ideal. You can see this in action in the video, we make the car drift around, but show the model where the ideal trajectory is regardless of its own position. Let’s apply the earlier thought experiment to this model. If the car is 10 cm away from the center of the lane the model will now predict a trajectory closer to the center of the lane. If we let a controller execute on that trajectory, it will recover. Cheaters, a new problem The solution above seems like it works, but it has a new hidden problem. It works in simulation, but only in simulation. Our simulation method just warps the image to make it seem as though the car is moving sideways. This method introduces a lot of artifacts. We consider the ideal trajectory to be the one the human actually drove. This means that during simulation, when you are on the ideal path the image is unwarped. So to figure out where the ideal trajectory is, all the model has to learn is how much and in which direction the image is warped. It is much easier for the model to learn the warping artifacts and see how the image is warped than to learn where humans would drive for a given scene. So the end result is a model that just predicts you are on the ideal trajectory if the video doesn’t look warped. When you actually use this model in real life, it ends up always predicting you are on the ideal trajectory and never recovering from any mistakes. These models just learned to do well in the tests, but never learned the real task we wanted them to, so we call them cheaters. The KL Solution We need to make sure our models learn where to drive and not just how to exploit simulator artifacts. To solve this, we could try to make a better simulator that doesn’t have obvious artifacts, but this is incredibly hard. Instead, we try to make the model blind to the artifacts of the simulator. Pull the kale over the model’s eyes Imagine a feature vector that contains all the information of a certain road scene that is relevant to trajectory-planning, but no information about anything else, such as any potential simulator artifacts. If we just train the vision model with the simple approach described above and take the feature vector it outputs, that vector should contain most of the information relevant for trajectory-planning. This feature vector is uninterpretable to us, but we know it must contain that information because the policy models can generate good trajectories from it. Unfortunately, our tests indicate this vector does still contain information about how the image is warped. To remove that information we apply a KL-divergence loss to this feature vector at train-time. This loss makes the vision model minimize the information content in the feature vector. When this loss is weighted appropriately, the vision model will preserve information that is relevant for trajectory-planning since that is needed to do well on the trajectory-prediction loss function and reject everything else. The resulting feature vector only contains information relevant to the problem it is trained on, which is trajectory-planning on unwarped images. We can now train a new model in the simulator, we reuse the part from the vision model that compresses it to a minimal vector and keep that part frozen. We just train a new policy model. We want the model to have some temporal context, so we add a GRU block in the policy model as well. Now it will learn where it should drive without exploiting the warp. This model now works in simulation and real life. Try it! The 0.8.3 release of openpilot includes a toggle that disables the use of lanelines. In this mode openpilot will only use this e2e-learned trajectory for lateral control. Experience the magic yourself! Join us Are you interested in joining the comma research team? Try the hiring challenge! Harald Schäfer, CTO @ comma.ai

---

# The Zoox robotaxi rolls into San Francisco

## Unknown date - zoox.com

Source: https://zoox.com/journal/zoox-robotaxi-in-san-francisco

The Zoox robotaxi rolls into San Francisco What’s next in CA and beyond It’s been a busy year for Zoox as we’ve continued to make meaningful advancements toward launching our autonomous ride-hailing service. Now, we’re excited to share updates on the latest chapter of our journey. We have recently rolled out our robotaxi on the streets of San Francisco and expanded our Las Vegas geofence, significantly expanding operations in California and Nevada. “This marks our entrance with the robotaxi into our second commercial market,” said Aicha Evans, CEO. “Since 2017, our test fleet has autonomously navigated San Francisco streets with a safety driver. Now that we’ve passed all critical safety measures, we are excited to begin testing our robotaxi in this wonderful city.” San Francisco is our home Remaining true to our phased approach for new deployments, initial testing for our robotaxi will occur in a focused area: the vibrant SoMa neighborhood in San Francisco. During this testing phase, rides will not be available to the public. Zoox employees can take rides, helping to refine the experience. As testing progresses, we will expand our geofence to include additional neighborhoods throughout the city. “Zoox is the only company driving a purpose-built robotaxi on public roads without traditional manual controls,” said Jesse Levinson, Co-Founder and CTO. “This expansion marks our third city for our robotaxi operations, following Foster City and Las Vegas. We’ve rigorously tested and validated our AI stack in multiple cities over the last seven years in preparation for the deployment of our robotaxi fleet. From day one, our robotaxis will be leveraging a deep history and understanding of this area.” While some residents of the city may already be familiar with our test fleet, the Zoox robotaxi is unlike any other vehicle on the road today. Over the last few months, we’ve been popping up at events across San Francisco and Las Vegas, and we encourage the public to visit and see a Zoox robotaxi in person. We want to get to know our neighbors and answer questions about Zoox to ensure the community feels informed. Expanding our robotaxi operations in Las Vegas We are making significant progress towards welcoming our first public riders in Las Vegas in 2025. We've recently deployed the robotaxi in a larger geofence in Las Vegas, including The Strip and surrounding areas. The Strip is one of the most highly trafficked areas in Las Vegas and is full of complex driving scenarios. These include areas of more than eight lanes of traffic with multiple turning lanes, high speeds, heavy pedestrian traffic, and large-scale intersections. Deploying our robotaxi in this dynamic area will provide invaluable learnings for our AI stack and service as we prepare Zoox for its first riders. The first to ride And speaking of first riders, as we prepare to welcome public riders in Las Vegas in the new year, we will do so through an early rider program. These select riders will have the opportunity to ride with Zoox for free, providing valuable feedback. If you are interested in being one of our early riders, please sign up for our newsletter and follow Zoox on social media to receive updates as available. We can’t wait to give you a ride! Last updated on Nov 11, 2024

---

# Staff Engineer, Cloud Infrastructure Performance | Cruise

## 2129-01-01 - www.getcruise.com

Source: https://www.getcruise.com/careers/jobs/2972129/

Cruise Rides Safety Technology Support About Join the waitlist Give us feedback Staff Engineer, Cloud Infrastructure Performance San Francisco, CA US Remote US Remote Apply Back to Job List

---

# Cruise (autonomous vehicle) - Wikipedia

## 2024-06-25 - en.m.wikipedia.org

Source: https://en.m.wikipedia.org/wiki/Cruise_(autonomous_vehicle)#:~:text=Founded%20in%202013%20by%20Kyle,autonomous%20subsidiary%20of%20General%20Motors.

37°46′12″N 122°24′35″W / 37.7699°N 122.4098°W Cruise LLC is an American self-driving car company headquartered in San Francisco, California. Founded in 2013 by Kyle Vogt and Dan Kan,[4][5][6] Cruise tests and develops autonomous car technology. The company is a largely autonomous subsidiary of General Motors.[7] Following a series of incidents, it suspended operations in October 2023, and Kyle Vogt resigned as CEO in November 2023.[8] The company began returning its vehicles to public roads in May 2024.[9] On June 25, 2024, the company named Marc Whitten as its new CEO. History editCruise initially focused on developing direct-to-consumer kits to retrofit vehicles with limited self-driving capabilities.[10] The earlier generation of Cruise technology, RP-1, offered an autonomous on-demand feature available for the Audi A4 or S4 (2012 or later). The $10,000 kit will eventually retrofit all vehicles into a highway autopilot system. Ultimately, Cruise determined that the greater challenge lay in conquering city driving. In January 2014, the company decided to abandon the RP-1 and produce a fully autonomous vehicle using the Nissan Leaf. In 2015, Cruise changed its strategy and began writing software for fully self-driving vehicles.[10] The brand philosophy urges car owners to engage in shared ownership instead of individual ownership to reduce environmental damage, the number of accidents, and congestion in big cities.[11] Cruise received a permit to test self-driving vehicle technology from the California Department of Motor Vehicles in June 2015.[12] After it successfully graduated from Y Combinator, a startup accelerator that mentors up-and-coming entrepreneurs, Cruise was acquired by General Motors in March 2016.[13] The amount was undisclosed, and reports have estimated the number from "north of $500 million",[14] to $580 million[15] to over $1 billion.[13] Cruise forms the core of GM's self-driving efforts.[16] Upon acquisition, Cruise had around 40 employees.[10] In a September 2016 interview with Darrell Etherington at the San Francisco TechCrunch Disrupt conference, Vogt confirmed that the company had over 100 employees.[17] Cruise's current headcount is unknown, but multiple outlets have reported that Cruise has continued to grow rapidly. In June 2017, Mary Barra stated that Cruise has close to 200 employees.[18] Industry observers have noted, and GM CEO Mary Barra has stated, that GM allowed Cruise to remain responsible for both technology and commercialization, giving Cruise independence to avoid the pitfalls common when a large company acquires a technology startup.[7] Acquisition by GM and investments editSince becoming part of General Motors in March 2016,[19] Cruise has been working on developing software and hardware to make fully autonomous vehicles using modified Chevrolet Bolts. In April 2017, GM announced plans to invest $14 million to expand Cruise operations in California, adding an estimated 1,163 full-time employees by 2021.[20][14] In May 2018, Cruise announced that SoftBank Vision Fund would invest $2.25 billion into the company, along with another $1.1 billion from GM itself.[21] In October 2018, Cruise announced that Honda would be investing $750 million into the company, followed by another $2 billion over the next 12 years.[22] In November 2018, the company got a new CEO, Dan Ammann, who had been a president of GM before accepting this position.[23] Cruise raised an additional $1.15 billion in new equity in May 2019, bringing its total valuation to $19 billion.[24][25] In January 2021 Microsoft, Honda and institutional investors invested a further $2 billion in combined new equity bringing the valuation to $30 billion.[26] In March 2021, Cruise acquired Voyage, a self-driving startup that was spun off of Udacity.[27] In March 2022, GM said that it is acquiring Softbank Vision Fund 1's equity ownership in Cruise for $2.1 billion. GM is also making an additional $1.35 billion investment in Cruise.[28] Robotaxi testing and permits editIn September 2021, Cruise received a permit from the California Department of Motor Vehicles to provide driverless taxi rides in the state. The permit allows the company to provide the service without the inclusion of safety drivers - staff that would accompany the vehicle and take control of it if necessary.[29] In November 2021, Cruise co-founder Kyle Vogt took the first driverless taxi drive in the company's history.[30] In February 2022, Cruise announced that it is now open to the public.[31][32] Also in February 2022, Cruise petitioned U.S. regulators (NHTSA) for permission to build and deploy a self-driving vehicle without human controls.[33] At the end of the month, Kyle Vogt was officially named CEO.[34] In June 2022, Cruise received California's first Driverless Deployment Permit, allowing it to charge fees for its service.[35] This was the first company to offer rides without a driver in a major public city. The company recalled and updated software in 80 self-driving vehicles in August, following a crash in June.[36] As of September 2022, the company operated 100 robotaxis in San Francisco, and announced their intentions to increase the size of their fleet to 5,000. However, this drew some criticism due to the potential for this to increase traffic within the city.[37] In September 2022, the company announced it would expand its service to Phoenix, Arizona, and Austin, Texas, within three months, with the goal of adding $1 billion in revenue by 2025.[38] The Verge reported that the company lost $561 million in Q1 2023, with most of the $30 million in revenue coming from interest and other non-operating sources of income, but said it remains on the path to reach $1 billion in revenue by 2025 and $50 billion by 2030. The article noted Cruise CEO Kyle Vogt said a small portion of the company's fleet offered driverless rides 24 hours a day across the entire city of San Francisco.[39] Suspension of operations editIn October 2023, following a barrage of safety concerns and incidents since Cruise received approval in August 2023 for round-the-clock robotaxi service in San Francisco,[40] the California DMV suspended Cruise's self-driving car permits following an investigation of a pedestrian collision, effective immediately.[41] Three days later, Cruise announced it would temporarily halt robotaxi operations nationwide.[42] Outside of California, the voluntary nationwide scope expansion suspended services in Arizona (Phoenix), Texas (Austin, Dallas, and Houston), and Florida (Miami).[43] On November 8, Cruise announced a recall of 950 autonomous vehicles, to be implemented by an over-the-air software update, as a result of the pedestrian collision.[44] The next day, Cruise announced it had laid off an unspecified number of contingent workers who had supported autonomous vehicle operations; at the time, Cruise had an estimated 4,000 full-time employees.[45] On November 19, CEO Kyle Vogt announced his resignation from Cruise in the wake of the suspension.[8] GM appointed Mo Elshenawy, Cruise’s executive vice president of engineering, and Craig Glidden, GM's General Counsel, as co-presidents.[46] A subsequent internal investigation conducted by Quinn Emanuel Urquhart & Sullivan and released in January 2024 concluded that the DMV's suspension of Cruise's operations was largely the result of deficient leadership at Cruise. At that time, the company remained under investigation by the U.S. Department of Justice and the U.S. Securities and Exchange Commission.[47] New CEO editOn June 25, 2024, the company announced Marc Whitten, a former Amazon and Microsoft executive, as its new CEO.[48] Products editCruise RP-1 editThe first product offered by Cruise was the RP-1, announced in June 2014 as a kit to be available in 2015. The RP-1 was priced at US$10,000, including installation, and was intended to be retrofitted to 2012 and newer Audi A4 and S4. The RP-1 package included a roof-mounted "sensor pod" with cameras, radar, GPS, inertial sensors, and on-board computer linked to steering, throttle, and brake actuators. Once activated on a limited-access highway, the RP-1 would maintain speed and following distance and keep the car within its lane. Pre-orders were limited to 50 units via a US$1,000 reservation/deposit fee.[49] However, Cruise shifted its strategy in 2015 to focus on the creation of a fully autonomous vehicle platform rather than a retrofit kit and never released the RP-1.[50][51] Cruise initially purchased Nissan Leaf battery-electric vehicles to test automated driving systems,[52] but these were not tested after December 2016.[53] Cruise AV editThe Cruise AV is a Chevy Bolt-based autonomous vehicle; the first generation (G1) were modified by Cruise in San Francisco while the subsequent second and third generations (G2 and G3) are manufactured at the Orion Township assembly plant in Michigan. The Cruise AVs feature "drive control algorithms and artificial intelligence created by Cruise."[54] The Cruise AV uses Lidar, radar, and camera sensors; according to Cruise, 40% of its hardware is unique to self-driving.[55] In 2016, Cruise was conducting testing with a fleet of approximately 30 self-driving G1 Cruise AVs.[56] By June 2017, GM and Cruise had produced an estimated 180 G1 and G2 Cruise AV self-driving vehicles after GM announced the mass production of 130 new G2 Cruise AVs.[57] In 2017, Cruise was conducting testing on public roads with Cruise AVs in San Francisco, Scottsdale, Arizona, and the metropolitan Detroit area. In early 2017, Cruise released a series of videos showing its self-driving vehicles navigating the streets of San Francisco.[58] In an interview with Fortune in July 2017, Vogt described the videos as "the most technically advanced demonstrations of self-driving cars that have ever been put out there in public."[59] Also in July 2017, Cruise announced "Cruise Anywhere," a program for San Francisco-based employees to use self-driving cars as a rideshare service. In October 2020, the California Department of Motor Vehicles granted a permit to Cruise for testing fully driverless vehicles.[60] Cruise began testing Cruise AVs without a human safety driver present on the streets of San Francisco in December 2020.[61] In September 2021, Honda started testing program toward launch of Level 4 mobility service Business in Japan, using the G3 Cruise AV.[62] Cruise Origin editIn January 2020, the company exhibited the Cruise Origin, a Level 4–5 driverless vehicle,[63] intended to be used for a ride hailing service.[64] The Origin is purpose-built as a self-driving vehicle, rather than retrofitted from a non-autonomous vehicle, and contains no manual steering controls.[65] Costing approximately $50,000 to manufacture at scale,[66] the vehicle is all-electric and designed to have a lifespan of 1,000,000 miles (1,600,000 km).[67] Cruise announced that future Origin vehicles would be manufactured at GM's Detroit-Hamtramck plant.[68][69] It is built on the GM BEV3 platform using Ultium battery technology.[70] In January 2021, Honda announced a partnership with Cruise to bring the Origin to Japan as part of Honda's future Mobility as a Service (MaaS) business.[71] By September 2022, a prototype of the Japanese version of the Cruise Origin for Tokyo was completed and started testing.[72] In May 2021, Cruise announced they expected mass production of the Origin driverless shuttle would commence in 2023.[73] In June 2021, Cruise announced it had secured a $5 billion line of credit from General Motors to assist with commercialization and that it had begun assembly of 100 pre-production Origin vehicles for validation testing.[74] Prototype Origin vehicles were spotted testing with human operators at GM-owned proving grounds by July 2022.[75] The State of California issued testing permits to Cruise in June 2021, allowing the company to provide limited revenue taxi service without human drivers.[76] In February 2022, General Motors and Cruise announced they had petitioned NHTSA for permission to build and deploy the Cruise Origin.[77][78] Because the Origin does not have manual controls, the NHTSA must issue an exception to the Federal Motor Vehicle Safety Standards to permit operation on public roads;[79] production was ready to begin by February 2023,[80] pending approval of the exception, which had not been granted by September 2023.[81] Along with the suspension of Cruise's operations, GM paused production of the Origin in November 2023.[82] Regulation editCruise received a permit from the California Public Utilities Commission (CPUC) in June 2022, allowing them to operate revenue taxi services in San Francisco using cars without a human driver during limited hours in certain areas of the city.[35] Cruise and Waymo later submitted applications to the CPUC, seeking to expand the commercial operating hours for their robotaxi services in San Francisco. In January 2023, the city of San Francisco wrote to the CPUC, requesting that the applications be denied: "in the months since the Initial Approval [of autonomous taxi services in June 2022], Cruise AVs have made unplanned and unexpected stops in travel lanes, where they obstruct traffic and transit service, and intruding into active emergency response scenes, including fire suppression scenes, creating additional hazardous conditions."[83][84] Although draft resolutions approving the expansion petitions were posted by the CPUC in May 2023,[85] the hearing was delayed from June 29 to July 13, pending further review.[86] Before the scheduled July 13 hearing was held, a group named Safe Street Rebel published a viral video to social media, showing that an autonomous vehicle could be disabled temporarily without permanent damage by placing a traffic cone on it and vowing to conduct a "Week of Cone" to demonstrate their concerns.[87][88] Cruise released a statement in response: "Intentionally obstructing vehicles gets in the way of [efforts to provide free rides, meal deliveries, and retrieve food waste] and risks creating traffic congestion for local residents."[89] The CPUC delayed the hearing for the permit expansions again to August 10.[90] At that hearing, the CPUC approved the scope expansion by a 3–1 vote, allowing both Cruise and Waymo to offer robotic taxi services at all hours throughout San Francisco.[91] On October 24, 2023, the California Department of Motor Vehicles suspended Cruise's permit to test and operate autonomous vehicles without a safety driver.[92] According to the suspension order, during their initial meeting with the California DMV on October 3, Cruise had shown a video captured by the AV's onboard cameras that depicted events up to the first complete stop after the Cruise AV struck the pedestrian; Cruise personnel neither described nor showed the remaining recording of the "pullover maneuver" that happened next, during which the pedestrian was dragged for 20 ft (6.1 m) at 7 mph (11 km/h). After learning about the dragging from another government agency, the DMV requested and received the full recording from Cruise. A spokesperson for Cruise disputed the dragging footage was withheld, saying the complete video was shown multiple times.[93] In its press release, the California DMV stated it had determined "the manufacturer's vehicles are not safe for the public's operation" and added "the conduct of autonomous vehicle testing on public roads by the manufacturer [makes] an unreasonable risk to the public". The suspension is effective immediately and does not have a set expiration; the permit will be reinstated after Cruise completes unspecified required actions.[94] The CPUC also suspended Cruise's permit to provide autonomous revenue taxi services the same day, following the DMV.[95] In May 2024, Cruise began returning its vehicles to publics roads, although an investigation by the National Highway Traffic Safety Administration (NHTSA) into potential flaws in Cruise vehicles remains pending.[9] Incidents editInterference with police and fire services editIn April 2022, the San Francisco Police Department (SFPD) stopped a Cruise AV for driving at night without its headlights on. The AV was empty, operating without any human safety attendants or passengers. The vehicle pulled over for SFPD ahead of an intersection, then proceeded across the intersection when an officer walked away from it;[96] on the other side of the intersection, the vehicle stopped again and turned on its hazard lights.[97] According to Cruise, the vehicle operated as intended, moving to the "nearest safe location" for the traffic stop in response to direction from Cruise personnel after the SFPD officer was clear of the vehicle.[97] Also in April 2022, an empty Cruise AV blocked the path of a San Francisco Fire Department (SFFD) truck responding to a fire at approximately 4 AM; the fire truck was unable to pass a garbage truck doubled-parked in the lane by using the lane for oncoming traffic, as the AV was occupying the oncoming lane. The Cruise AV had stopped and yielded to the fire truck, but was unable to pull to the right to clear the oncoming lane because of parked cars.[98] While a human might have reversed to clear the lane, the Cruise AV did not move out of the way of the fire truck. San Francisco city officials filed a report to the CPUC, stating that "this incident slowed SFFD response to a fire that resulted in property damage and personal injuries."[99] In June 2023, a video was taken of a Cruise car appearing to block police and fire services from responding to a mass shooting in San Francisco. Cruise denied that the car had blocked the road, stating that emergency response vehicles "were able to proceed around our car". The police and fire departments declined to comment.[100] The San Francisco Fire Department reported there had been 39 incidents between January and June 2023 where Cruise and Waymo robotaxis had blocked first responders.[101] In August 2023, a San Francisco firetruck collided with a Cruise car while responding to an emergency call with lights and sirens active.[102] As a result of the incident, the California Department of Motor Vehicles requested that "Cruise to immediately reduce its active fleet of operating vehicles by 50% until the investigation is complete and Cruise takes appropriate corrective actions to improve road safety".[103] City of Austin police and fire departments noted 12 "near miss" incidents occurred with Cruise vehicles between July and November 2023; Austin FD requested that Cruise set up an avoidance area around Fire Station 2, and Austin PD noted the vehicles were unable to recognize hand signals that conflicted with traffic signals.[104] Other traffic collisions and clustering editOn June 3, 2022, a Cruise AV taxi carrying three backseat passengers was involved in an accident with a Toyota Prius after making an unprotected left turn. According to Cruise, "occupants of both vehicles received medical treatment for allegedly minor injuries". According to GM, the Prius was speeding at the time of the accident and was in the wrong lane. In the aftermath of the incident, Cruise temporarily changed the vehicles' programming to make fewer unprotected left turns.[105] On June 29, 2022, nearly twenty Cruise AVs blocked traffic for two hours by clustering at the intersection of Gough and Fulton in San Francisco.[106][107] A Cruise employee sent an anonymous letter to the CPUC, asserting that Cruise loses communication with the automated vehicles "with regularity", sometimes requiring a tow truck for recovery.[108] Additional documented occurrences of immobilized Cruise vehicles in 2022 include May 18 (fleet-wide communications loss), June 21 (Tenderloin),[108] and September 22 (two incidents; one near Sacramento and Leavenworth, the other near Geary and Franklin).[109] San Francisco has recorded 28 incidents reported by 9-1-1 involving autonomous vehicle failures between May 29 and September 5, 2022.[110]: 4, 31 In October, US News and World Report reported that Cruise autonomous taxis had blocked traffic in San Francisco on several occasions.[111] In March 2023, two Cruise AVs operating without passengers drove through a blocked off intersection in San Francisco, and got tangled with downed power lines normally used by Muni trolleybuses.[112] The next day, a passengerless Cruise AV rear ended a Muni bus on Haight Street.[113] Disengagement and remote assistance editIn November 2023, The New York Times reported that on average, according to two people familiar with Cruise's operations, a remote operator had to intervene for every 2+1⁄2 to 5 mi (4.0 to 8.0 km) driven.[114] Vogt confirmed that its vehicles made a call for remote assistance "2–4% of the time on average, in complex urban environments", but clarified that "of those, many are resolved by the AV itself before the human even looks at things". A spokesperson for Cruise added that a call remote assistance is triggered every 4 to 5 mi (6.4 to 8.0 km).[115] Brad Templeton reported that remote assistance did not mean real-time remote operation by a human, citing reporting from Cruise that stated 98% of remote assistance calls connected to a human operator within 3 seconds, and 80% of remote assistance calls were resolved without action by the human operator.[116] Cruise reported an average distance between disengagements of 95,900 mi (154,300 km) in 2022.[117] Detection editIn November 2023, The Intercept reported that Cruise vehicles were unable to consistently recognize and track children and large holes, according to a review of internal safety reports.[118] Pedestrian injuries editIn August 2023, a Cruise AV starting to proceed through an intersection at 1.4 mph (2.3 km/h) on a green light struck a pedestrian, who subsequently was transported by ambulance after complaining of knee pain.[119][120] At approximately 9:30 p.m. (PDT) on October 2, 2023, a pedestrian was struck by a driver traveling south on Fifth Street in San Francisco after crossing the intersection with Market Street. The driver continued without stopping after the pedestrian was tossed over the right side of the car into the adjacent lane, where a Cruise AV nicknamed "Panini", traveling alongside and operating without a safety driver, subsequently also struck and then dragged the pedestrian for approximately 20 ft (6.1 m), coming to a stop with a tire resting on the pedestrian's leg, pinning them beneath the AV. The Jaws of Life were used to free the pedestrian from underneath "Panini". The driver who initially struck the pedestrian fled the scene.[121][122] According to Cruise, the AV had attempted to avoid striking the pedestrian by first swerving to the right, then stopping immediately after a collision had been detected, followed by attempting to pull over to clear the road. The pedestrian was dragged during the pullover attempt.[123][124] Investigations editThe National Highway Traffic and Safety Administration (NHTSA) opened preliminary investigation PE22-014 on December 12, 2022, citing incidents in which the vehicles may have engaged "in inappropriately hard braking or [became] immobilized".[125][126] Three specific incidents of hard braking were described, all involving situations in which the automated driving systems, while operating under human supervision, initiated a hard braking maneuver in response to other road users approaching quickly from behind, resulting in the other road user striking the automated car.[125] Two letters were sent to Cruise on January 4, 2023, requesting relevant data.[127][128] The California DMV opened an investigation in August 2023 following "recent concerning incidents" involving Cruise AVs.[129] The investigation resulted in the immediate suspension of Cruise's permit to test and operate autonomous vehicles, effective October 24, 2023.[130] In response to the two collisions with pedestrians in August and October 2023, NHTSA opened preliminary investigation PE23-018 on October 16, 2023;[120][131] in addition, NHTSA identified two other incidents from publicly posted videos where AVs had encroached on pedestrians in or entering roadways.[132] See also editReferences edit- ^ "Cruise co-founder and CEO Kyle Vogt resigns". Retrieved November 20, 2023. - ^ "Cruise CEO Vogt Resigns at GM's Troubled Self-Driving Car Unit". - ^ Wayland, Michael (May 14, 2020). "GM's self-driving unit Cruise to cut 8% of staff". Retrieved December 11, 2020. - ^ Waters, Richard (November 29, 2018). "General Motors president to control Cruise self-driving unit". - ^ Clifford, Catherine (April 26, 2016). "This 29-Year-Old Entrepreneur Was Rejected by 35 Potential Employers. Now, He's the Co-Founder of a $1 Billion Startup. - ^ Gardner, Greg (April 15, 2016). "Former Cruise partner fires back in fight over GM deal". - ^ a b Michael Wayland (June 19, 2017). "GM lets its autonomous unit be autonomous". - ^ a b "Cruise CEO quits after alleged cover-up". Retrieved November 21, 2023. - ^ a b Thadani, Trisha; Duncan, Ian (May 24, 2024). "Major robotaxi firms face federal safety investigations after crashes". Retrieved May 26, 2024. - ^ a b c Erin Griffith (September 22, 2016). "Driven in the Valley: The Startup Founders Fueling GM's Future". - ^ "GM's Cruise unveils its first driverless vehicle". - ^ Horatiu Boeriu (September 19, 2015). "These carmakers have licenses to test autonomous cars in California". - ^ a b Dan Primack; Kirsten Korosec (March 11, 2016). "GM Buying Self-Driving Tech Startup for More Than $1 Billion". - ^ a b "GM's Cruise employees test-ride startup's robot cars". San Francisco Chronicle. - ^ Matthew DeBord (July 21, 2016). "GM paid a lot less for Cruise Automation than everyone thought". Baker; Carolyn Said (July 14, 2017). "How the Bay Area took over the self-driving car business". - ^ Life Tips from Kyle Vogt of Cruise at Disrupt SF. - ^ Cadie Thompson (June 28, 2017). "GM wants the era of self-driving cars to be led by women". - ^ "GM to Acquire Cruise Automation to Accelerate Autonomous Vehicle Development" (Press release). Retrieved November 10, 2023. - ^ "GM Announces More Than 1100 Jobs To Expand Cruise Automation Self-Driving Operations In California". California - Governor's Office of Business and Economic Development. Archived from the original on September 9, 2017. Retrieved September 8, 2017. - ^ "GM's Cruise gets $2.25B from SoftBank's Vision Fund, $1.1B from GM". Retrieved August 24, 2018. - ^ "Honda to Invest $2.75 Billion in GM's Self-Driving Car Unit". Retrieved October 5, 2018. - ^ "Cruise Automation taps GM president Dan Ammann as its new CEO". Retrieved November 30, 2018. - ^ Hawkins, Andrew J. "GM's self-driving division Cruise raises another $1.15 billion". - ^ Marshall, Aarian (May 7, 2019). "Cruise's $1 Billion Infusion Shows the Stakes in Self-Driving Tech". - ^ Wayland, Michael (January 19, 2021). "Microsoft is investing and partnering with GM's Cruise on self-driving cars". Retrieved February 10, 2021. - ^ Korosec, Kirsten (March 15, 2021). "Cruise acquires self-driving startup Voyage". Retrieved March 15, 2021. - ^ Korosec, Kirsten (March 19, 2022). "GM is buying out Softbank's stake in autonomous vehicle unit Cruise". Retrieved March 20, 2022. "Cruise gets the green light to give driverless rides to passengers in San Francisco". Retrieved October 1, 2021. - ^ Bellan, Rebecca (November 3, 2021). "Cruise launches driverless robotaxi service in San Francisco". Retrieved November 4, 2021. - ^ "Welcome, Riders". Archived from the original on February 2, 2022. Retrieved February 2, 2022. - ^ Vijayenthiran, Viknesh (February 2, 2022). "Cruise opens up driverless taxi service to public in San Francisco". Retrieved March 16, 2022. - ^ Shepardson, David (February 19, 2022). "GM seeks U.S approval to deploy self-driving vehicles". - ^ Crowe, Steve (February 28, 2022). "Cruise co-founder Vogt named CEO of autonomous driving company". Retrieved March 17, 2022. - ^ a b Kolodny, Lora (June 2, 2022). "Cruise gets green light for commercial robotaxi service in San Francisco". Retrieved June 3, 2022. - ^ Wayland, Michael (September 1, 2022). "GM's Cruise recalls and updates self-driving software in cars following crash". Retrieved September 1, 2022. - ^ McFarland, Matt (September 30, 2022). "GM's Cruise wants to add 5,000 more robotaxis to American streets. This city warns it could backfire | CNN Business". Retrieved December 15, 2022. - ^ Welch, David (September 12, 2022). "GM's Cruise Will Expand Robotaxi Service to Phoenix and Austin". "Cruise continues to burn GM's cash as robotaxis expand to daylight hours". Retrieved April 28, 2023. - ^ Bhuiyan, Johana (August 10, 2023). "San Francisco to get round-the-clock robo taxis after controversial vote". - ^ Field, Hayden (October 24, 2023). "California DMV suspends Cruise's self-driving car permits, effective immediately". - ^ Feuer, Will (October 27, 2023). "GM's Cruise Pauses All Driverless Operations After California Crackdown". The Wall Street Journal. Retrieved October 27, 2023. - ^ Shepardson, David (October 26, 2023). "GM Cruise unit suspends all driverless operations after California ban". "Cruise is recalling 950 driverless cars after one of its vehicles ran over a pedestrian". - ^ Korosec, Kirsten (November 9, 2023). "Cruise begins layoffs, starting with workers who supported driverless operations". - ^ Lu, Yiwen; Mickle, Tripp (November 19, 2023). Quits as the Driverless Carmaker Aims to Rebuild Trust". Retrieved November 29, 2023. - ^ Field, Hayden; Wayland, Michael (January 25, 2024). "Probe into GM's Cruise finds poor leadership, culture issues at center of accident response". Retrieved January 26, 2024. - ^ Wayland, Michael (June 25, 2024). "GM's Cruise names former Amazon, Microsoft Xbox executive as new CEO". Retrieved June 26, 2024. - ^ Strange, Adario (June 23, 2014). "Hands-Free Driving Coming to New Cars Starting in 2015". Retrieved July 5, 2022. - ^ Bergen, Mark (March 11, 2016). "Meet Kyle Vogt, the 'Robot Guru' Who Just Sold His Second Billion-Dollar Startup in Two Years". At some point [in 2015], Vogt shifted plans to focus on building a full self-driving platform. That strategy pits the startup against Google, which is, according to multiple industry sources, pitching a similar system to carmakers. - ^ Hyatt, Nabeel (March 11, 2016). "On Cruise, the hard things the hard way". Then, just a few months later, [Vogt] emailed to say he'd gotten the RP-1 highway autopilot fully working, only he was killing it. Oh, and he had something new to show me, I should come by. - ^ Ostojic, Sasha (December 22, 2016). Autonomous Vehicle Tester Program Annual Disengagement Report (PDF) (Report). California Department of Motor Vehicles. Archived from the original (PDF) on October 13, 2018. - ^ Boniske, Albert (December 29, 2017). Archived from the original (PDF) on February 11, 2018. - ^ Alan Ohnsman (April 4, 2017). "GM's Cruise Poised To Add 1,100 Silicon Valley Self-Driving Car Tech Jobs". - ^ "Technology / Our First-Gen AV". - ^ Darrell Etherington (September 14, 2016). "Cruise has around 30 self-driving test cars on roads right now". - ^ Melissa Burden (January 19, 2017). "GM's Cruise Automation releases self-driving Bolt video". Hawkins (January 19, 2017). "Watch GM's self-driving car navigate the streets of San Francisco". - ^ Full Throttle on Self-Driving Cars. "Cruise is now testing fully driverless cars in San Francisco". - ^ Said, Carolyn (December 9, 2020). "Cruise deploys true robot cars in S.F. — no backup drivers behind wheel". - ^ "Honda to Start Testing Program in September Toward Launch of Autonomous Vehicle Mobility Service Business in Japan" (Press release). - ^ Howard, Bill (January 23, 2020). "GM's Cruise Origin Is an Autonomous Vehicle From the Future". Retrieved December 14, 2020. - ^ Baldwin, Roberto (January 22, 2020). "Cruise Unveils Origin, a Self-Driving Vehicle with No Steering Wheel or Pedals". "Exclusive Look at Cruise's First Driverless Car Without a Steering Wheel or Pedals". - ^ Priddle, Alisa (January 22, 2020). "GM's Cruise Origin Self-Driving Pod Has No Steering Wheel, No Pedals, and No Driver". - ^ Wayland, Michael; Kolodny, Lora (January 23, 2020). "Debut of GM's Cruise Origin shows the future of ride-sharing, autonomous vehicles is a box". "GM will spend $2.2 billion to build electric and autonomous vehicles at Detroit plant". "GM rebrands its Detroit-Hamtramck plant as 'Factory Zero' for electric and autonomous vehicles". - ^ "GM Reveals New Ultium Batteries and a Flexible Global Platform to Rapidly Grow its EV Portfolio" (Press release). General Motors Newsroom. - ^ "Honda, Cruise and GM Take Next Steps Toward Autonomous Vehicle Mobility Service Business in Japan" (Press release). Retrieved April 5, 2021. - ^ "自動運転車両「クルーズ・オリジン」の試作車が完成、米国でテストを開始" [Prototype of self-driving car "Cruise Origin" completed, started testing in the United States]. Retrieved November 25, 2022. - ^ "Cruise expects GM to begin production of new driverless vehicle in early 2023". Retrieved June 21, 2021. - ^ "GM-backed Cruise secures $5 billion credit line as it prepares to launch self-driving robotaxis (www.cnbc.com)". - ^ Lopez, Jonathan (July 22, 2022). "Cruise Origin Spied Undergoing Testing". Retrieved September 19, 2023. - ^ "California will allow GM-backed Cruise to transport passengers in driverless test vehicles (www.cnbc.com)". - ^ David Shepardson (February 19, 2021). Retrieved April 18, 2022. - ^ Brodkin, Jon (February 22, 2022). "GM seeks US approval to deploy self-driving car without a steering wheel". - ^ Shepardson, David (July 12, 2023). "US to decide on GM request to deploy self-driving cars". "GM's plan to deploy self-driving Cruise Origin on hold as feds weigh exemption request". "Cruise 'just days away' from approval to mass-produce Origin robotaxis without steering wheels". - ^ Bellan, Rebecca (November 6, 2023). "GM halts production of Cruise Origin robotaxi amid suspended operations". - ^ "Protest of Cruise LLC Tier 2 Advice Letter" (PDF). San Francisco Municipal Transportation Agency. Retrieved February 1, 2023. - ^ Ingram, David (January 27, 2023). "Two companies race to deploy robotaxis in San Francisco. The city wants them to hit the brakes". - ^ Bellan, Rebecca (May 18, 2023). "Cruise, Waymo near approval to charge for 24/7 robotaxis in San Francisco". Retrieved July 27, 2023. - ^ Truong, Kevin; Mojadad, Ida (June 27, 2023). "Robotaxi Service Expansion in San Francisco Delayed by State Regulators". The San Francisco Standard. - ^ Taylor, Chloe (July 10, 2023). "Angry San Franciscans are sabotaging Waymo and Cruise robotaxis with one simple trick". - ^ Stumpf, Rob (July 6, 2023). "Grumpy Locals Are Sabotaging Cruise and Waymo Robotaxis With Traffic Cones". - ^ Bellan, Rebecca (July 6, 2023). "Robotaxi haters in San Francisco are disabling the AVs with traffic cones". - ^ Guaglianone, Carmela (July 11, 2023). "State vote on expanding Cruise, Waymo service in SF delayed — again". San Francisco Examiner. "Robotaxis score a huge victory in California with approval to operate 24/7". - ^ Korosec, Kirsten (October 24, 2023). "California DMV immediately suspends Cruise's robotaxi permit". Retrieved October 24, 2023. - ^ Gordon, Aaron (October 24, 2023). "Cruise Self-Driving License Revoked After It Withheld Pedestrian Injury Footage, DMV Says". Retrieved October 25, 2023. - ^ "DMV statement on Cruise LLC suspension" (Press release). - ^ Bellan, Rebecca (October 24, 2023). "CAlifornia agency pulls Cruise's commercial robotaxi permit following DMV action". - ^ Hall, Kalea (April 11, 2022). "What happens when police pull over a driverless car? Watch this video and see". - ^ a b Wiggers, Kyle (April 11, 2022). "Autonomous Cruise car encounter with police raises policy questions". - ^ Templeton, Brad (May 27, 2022). Says Cruise Robotaxi Did Not Yield Properly And Delayed Fire Truck". - ^ Marshall, Aarian (May 27, 2022). "An Autonomous Car Blocked a Fire Truck Responding to an Emergency". - ^ Parker, Jordan; Swan, Rachel (June 10, 2023). "Cruise self-driving car appears to block S.F. Retrieved June 15, 2023. - ^ Kukura, Joe (June 23, 2023). "SF Fire Chief Fumes Over Self-Driving Robotaxis Blocking First Responders Dozens of Times This Year". Archived from the original on July 27, 2023. - ^ Rodríguez, Gloria; Melendez, Lyanne (August 19, 2023). "VIDEO: Driverless Cruise car struck by SF firetruck, injuring passenger, company says". - ^ Rumpf-Whitten, Sarah (August 19, 2023). "California DMV requests Cruise to halve driverless car fleet after collision with firetruck in San Francisco". - ^ Marshall, Aarian (November 9, 2023). "GM's Cruise Rethinks Its Robotaxi Strategy After Admitting a Software Fault in Gruesome Crash". - ^ Scott, Victoria (July 8, 2022). "Self-Driving Cruise Taxi Crashes With Passengers On Board". Retrieved July 9, 2022. - ^ Bindman, Ariana (July 1, 2022). "'Surreal': Cruise's driverless cars block traffic for nearly two hours in San Francisco". - ^ Bellan, Rebecca (June 30, 2022). "Cruise robotaxis blocked traffic for hours on this San Francisco street". - ^ a b Marshall, Aarian (July 8, 2022). "Cruise's Robot Car Outages Are Jamming Up San Francisco". Retrieved July 8, 2022. - ^ Bartlett, Amanda (September 26, 2022). "Multiple driverless Cruise cars block traffic in San Francisco". - ^ "Comment for Docket # NHTSA-2022-0067 (General Motors)". City and County of San Francisco. The City has no information about whether the total of 28 incidents reported to 9-1-1 between May 29 and September 5, 2022 reflect recurrence of the same issues, or whether each reflects a unique issue. The large majority of these reported incidents occurred between the hours of 10 pm and 6 am when Cruise has been authorized by the California Public Utilities Commission to offer commercial service in 30 driverless Cruise AVs. We have identified an additional 20 incidents posted on social media during this three-month time period. It is reasonable to assume that the number of incidents reported to 9-1-1 and posted on social media or observed by San Francisco Police Officers are a fraction of actual travel lane road failures because few people are on the street during these hours to observe and make such reports. - ^ Motavalli, Jim (October 14, 2022). "GM's Self-Driving Cruise Cars are Causing Traffic Snarls". US News & World Report. - ^ Whiting, Sam (March 23, 2023). "Self-driving Cruise cars that tangled with S.F. Muni lines had no passengers, company says". - ^ Mayer, Phil (March 24, 2023). "Cruise car appears to rear-end Muni bus". - ^ Mickle, Tripp; Metz, Cade; Lu, Yiwen (November 3, 2023). "G.M.'s Cruise Moved Fast in the Driverless Race. - ^ Kolodny, Lora (November 6, 2023). "Cruise confirms robotaxis rely on human assistance every four to five miles". - ^ Templeton, Brad (November 7, 2023). "Cruise Reports Lots Of Human Oversight Of Robotaxis, Is That Bad?". - ^ Juliussen, Egil (April 11, 2023). "Waymo, Cruise Dominate AV Testing". - ^ Biddle, Sam (November 6, 2023). "Cruise knew its self-driving cars had problems recognizing children—and kept them on the streets". - ^ "Incident Report, ID 30412-6175-1" (PDF). United States Department of Transportation, National Highway Traffic Safety Administration. Archived from the original (PDF) on October 17, 2023. - ^ a b Goswami, Rohan (October 17, 2023). "Federal regulators open probe into Cruise after pedestrian injury reports". - ^ "Hit-and-run driver strikes pedestrian, tossing her into path of Cruise car in San Francisco". - ^ Parker, Jordan; Mishanec, Nora (October 2, 2023). "Driver hits woman in S.F., then Cruise driverless car runs her over; photo shows victim trapped". - ^ "A detailed review of the recent SF hit-and-run incident". - ^ "Incident Report, ID 30412-6395-1" (PDF). - ^ a b Haugh, Thomas (December 12, 2022). "ODI Resume: PE 22-014" (PDF). National Highway Traffic and Safety Administration. - ^ Korosec, Kirsten (December 16, 2022). "Cruise's autonomous driving tech comes under scrutiny from safety regulators". - ^ "PE22-014 to investigate allegations of Automated Driving System (ADS) equipped vehicles operated by Cruise LLC (Cruise) engaging in inappropriately hard braking while operating in the specified Operational Design Doman (ODD)" (PDF). - ^ "PE22-014 to investigate allegations of Automated Driving System (ADS) equipped vehicles operated by Cruise LLC (Cruise) becoming immobilized while operating in the specified Operational Design Doman (ODD)" (PDF). - ^ San Juan, Diana (August 18, 2023). "California DMV asks Cruise to reduce number of driverless cars in San Francisco". - ^ Liedtke, Michael (October 4, 2023). "California regulators suspend recently approved San Francisco robotaxi service for safety reasons". - ^ Haugh, Thomas (October 16, 2023). "ODI Resume: PE 23-018" (PDF). - ^ Chapman, Michelle (October 17, 2023). "US regulators investigate GM's Cruise division over incidents involving pedestrians in roadways"

---

# GM Cruise probe finds poor leadership at center of accident response

## 2024-01-25 - www.cnbc.com

Source: https://www.cnbc.com/2024/01/25/gm-cruise-probe-finds-poor-leadership-at-center-of-incident-response.html

Culture issues, ineptitude and poor leadership at General Motors' Cruise autonomous vehicle unit were at the center of regulatory oversights and coverup concerns that have plagued the company since October, according to the findings of a third-party probe. The report addresses, in part, controversy that has swirled around Cruise since an Oct. 2 accident in which a pedestrian in San Francisco was dragged 20 feet by a Cruise robotaxi after being struck by a separate vehicle. Results of the investigation, which reviewed whether Cruise representatives misled investigators or members of the media in discussing the incident, were published Thursday in a 105-page report. Despite the findings, which pointed to widespread issues with company culture, the third-party probe found that the evidence to date "does not establish that Cruise leadership or personnel intended to deceive or mislead regulators" during briefings a day after the accident, according to a summary of the report released by Cruise. Cruise remains under investigation by several entities, including the U.S. Department of Justice and the U.S. Securities and Exchange Commission. Several Cruise leaders and employees — most of whom are no longer employed by the company — attempted to show regulators a video of the incident, according to the findings, but were only able to do so in one of several initial meetings due to connection or "video transmission issues." Although the intent to share the information had been there, the report found, the Cruise representatives subsequently failed to properly inform some regulators or officials of everything that occurred. "The problem is that when the video froze, literally and figuratively, the Cruise employees froze in the moment, and nobody thought to speak up and fill in the detail," a person close to the investigation told CNBC. Some employees also failed to update or correct company statements that omitted such information and attempted to deflect blame on the human hit-and-run driver who initially struck the pedestrian. The report outlines multiple instances in which then-CEO and co-founder Kyle Vogt, who resigned in late November, made the final calls to withhold information, specifically regarding media. "This conduct has caused both regulators and the media to accuse Cruise of misleading them," the report said. "The reasons for Cruise's failings in this instance are numerous: poor leadership, mistakes in judgment, lack of coordination, an 'us versus them' mentality with regulators, and a fundamental misapprehension of Cruise's obligations of accountability and transparency to the government and the public." Quinn Emanuel Urquhart & Sullivan, the business law firm that GM and Cruise retained to conduct the three-month investigation, interviewed 88 Cruise employees and reviewed more than 200,000 documents, including emails, texts, Slack messages and more. The investigation was led by former federal prosecutor John Potter, a San Francisco-based partner and co-lead of Quinn Emanuel's corporate investigations group. The firm is known for representing high-profile celebrities and business owners, including Tesla CEO Elon Musk. Cruise 'accepts' report Since the incident, Cruise's robotaxi fleet has been grounded. Local and federal governments have launched their own investigations. Cruise leadership has been gutted: Its cofounders, including Vogt, resigned and nine other leaders were ousted. And the venture laid off 24% of its workforce, as well as a round of contractors. Cruise said it "accepts" the conclusions found in the report. The San Francisco-based company, of which GM owns about 80%, said it will "act on all" recommendations and is "fully cooperating" with investigations by state and federal agencies following the Oct. The company said Thursday that investigations or inquiries into the incident include those by the California DMV, California Public Utilities Commission, National Highway Traffic Safety Administration, U.S. Department of Justice and U.S. "It was a fundamentally flawed approach for Cruise or any other business to take the position that a video of an accident causing serious injury provides all necessary information to regulators and otherwise relieves them of the need to affirmatively and fully inform these regulators of all relevant facts," the Quinn Emanuel findings said. Read more A separate investigation by engineering consulting firm Exponent Inc. found the Cruise autonomous vehicle involved in the Oct. 2 incident "incorrectly classified the collision with the pedestrian as a side-impact collision, which led the AV to perform a subsequent pullover maneuver (to the outermost lane) instead of an emergency stop," according to the report. Exponent's results, which also found a semantic mapping error, were consistent with Cruise's analysis of the incident, according to the company. Cruise said it updated the software to address the underlying issues and filed a voluntary recall with the NHTSA in November. Future of Cruise? Cruise vehicles remain grounded in the U.S. A source familiar with the operations told CNBC the company is "committed" to relaunching operations but is currently focused on rebuilding trust with regulators and addressing other issues outlined in the report. Prior to the accident, Cruise was planning aggressive expansion of robotaxis outside its home market, where the majority of its vehicles operated. Cruise, which GM acquired in 2016, was considered to be among the leaders in autonomous vehicles alongside Alphabet-backed Waymo, outlasting many other companies that have abandoned the segment. After purchasing Cruise, GM brought on investors such as Honda Motor, SoftBank Vision Fund and, more recently, Walmart and Microsoft. However, in 2022, GM acquired SoftBank's equity ownership stake for $2.1 billion. GM CEO and Chair Mary Barra, who leads Cruise's board, said in December that the Detroit automaker is "very focused on righting the ship" at Cruise. The Quinn Emanuel report does not directly reference Barra. GM is mentioned several times. GM said in a statement the Quinn Emanuel report "confirms Cruise's actions following the incident on October 2 were not consistent with the company's values and fell far short of the justifiable expectations of regulators and the public." "We know that in order to successfully move forward, Cruise must do so in full partnership with regulators and the communities it serves. We remain committed to Cruise's vision and know this transformative technology will ultimately save lives," the company said Thursday.

---

# Cruise resumes manual driving | April 2024 | Cruise

## 2024-01-01 - www.getcruise.com

Source: https://www.getcruise.com/news/blog/2024/cruise-resumes-manual-driving-as-next-step-in-return-to-driverless-mission/

Blog Post 4.9.2024 Cruise resumes manual driving as next step in return to driverless mission Share Updated June 11th, 2024 As of 6/11, Cruise has resumed manual driving Phoenix, AZ, Houston and Dallas, TX. Supervised driving is underway in Phoenix and Dallas. Cruise was founded in 2013 with a clear and focused mission: to make transportation safer and more accessible. In the ten years that followed, we’ve worked hard to build advanced self-driving technology and provided hundreds of thousands of driverless trips to riders on complex city streets in San Francisco, Phoenix, Austin, Houston and elsewhere. In October 2023, we paused operations of our fleet to focus on rebuilding trust with regulators and the communities we serve, and to redesign our approach to safety. We’ve made significant progress, guided by new company leadership, recommendations from third-party experts, and a focus on a close partnership with the communities in which our vehicles operate. We are committed to this improvement as a continuous effort. Looking to the next chapter, our goal is to resume driverless operations. As we continue working to rebuild trust and determine the city where we will scale driverless, we also remain focused on continuing to improve our performance and overall safety approach. To that end, Cruise is resuming manual driving to create maps and gather road information in select cities, starting in Phoenix. This work is done using human-driven vehicles without autonomous systems engaged, and is a critical step for validating our self-driving systems as we work towards returning to our driverless mission. This will help inform where we ultimately will resume driverless operations. Cruise has a strong history in Phoenix and it is home to a large number of Cruise employees. It’s a city that supports AV and transportation innovation, and Phoenix leaders strive to ensure the metro area is an incubator for advanced technology. We plan to expand this effort to other select cities as we continue to engage with officials and community leaders. Adaptive Fleet Learning Cruise’s AV stack is based on AI technology that learns from information gathered through our driving experience and retrains and evolves our models continuously. The fleet learns from every intersection, construction zone, and road sign it encounters, and applies that knowledge to other environments and scenarios – much the same as a human driver learns, but with far more data and the ability to impart that continuous learning across the entire fleet. During our operational pause over the last few months, Cruise maintained ongoing and extensive testing in complex, dynamic simulated environments and on closed courses, enabling continuous retraining and improvement. Now, we are building on that work to create high-quality semantic maps and gather road information to ensure future operations meet elevated safety and performance targets. And because no two cities are the same, we plan to conduct this manual and supervised driving in multiple cities - starting with Phoenix - to expose our AVs to a diverse set of driving environments and conditions as we prepare for future driverless service. Steps Prior to Driverless Operations The first step is identifying high fidelity location data for road features and map information like speed limits, stop signs, traffic lights, lane paint, right turn only lanes and more. Having current and accurate information will help an autonomous vehicle understand where it is and the location of certain road features. We also measure our perception and prediction systems against our elevated performance criteria, using trained safety drivers as a benchmark. At this stage, no autonomous systems are engaged and the vehicles will not carry public passengers. Next, we’ll validate our AV’s end-to-end performance against our rigorous safety and AV performance requirements through supervised autonomous driving on public roads, in addition to the ongoing simulation and closed course driving we do. During this phase, the Cruise vehicles will drive themselves and a safety driver is present behind the wheel to monitor and take over if needed. Safety is the defining principle for everything we do and will guide our progress through this process. As we begin this work, we have requirements in place that not only cover the safety criteria, functions and roadworthiness of the vehicle, but also include robust incident response protocols and extensive training and ongoing performance monitoring for the operators behind the wheel. Commitments to Safety and Our Communities Over the past several weeks we have communicated directly with officials, first responders, and community leaders in cities we’ve previously operated in to share updates on our path forward. We are committed to safely deploying our technology in close collaboration with officials and communities at every step. Our goal is to earn trust and build partnerships with the communities such that, ultimately, we resume fully driverless operations in collaboration with a city. As part of our commitment to improve our operations Cruise has implemented the following measures: Established new leadership, and engaged more closely with experienced advisors from GM to support safety, legal, regulatory, and communications functions. Hired an experienced chief safety officer to guide improved safety processes and procedures throughout the organization. Established a cross-disciplinary regulatory team to guide engagement with regulators in regard to incident reporting. Reviewed and strengthened key internal safety governance processes to incorporate more robust cross-functional review and leadership accountability. Work is also underway to establish important systems and processes for ensuring safe operations across the company including: Reforming and updating incident response and crisis management protocols to ensure more consistent, effective and transparent response. Renewing internal training and reinforcing safety culture systems. Reevaluating and reestablishing our safety target for supervised and driverless operations. Reengaging with first responders to facilitate ongoing trainings for each precinct and fire house in the areas we intend to operate in. Looking Ahead We believe AVs will save lives and significantly reduce the number and severity of accidents on America’s and Arizona’s roads every year. AVs will also improve lives - including creating convenient and safe transportation options for the elderly and those with disabilities. As we begin this journey, we look forward to partnering with local communities to jointly achieve our shared mission of making transportation safer for all.

---

# Cruise Board Appoints Marc Whitten as CEO | Cruise

## 2024-01-01 - www.getcruise.com

Source: https://www.getcruise.com/news/blog/2024/cruise-board-appoints-marc-whitten-as-ceo/

Blog Post 6.25.2024 Cruise Board Appoints Marc Whitten as CEO Share Also welcomes Nick Mulholland as Chief Communications & Marketing Officer Cruise today announced the appointment of Marc Whitten as the company’s Chief Executive Officer, effective July 16. Marc is a proven leader with decades of experience on the frontlines of technology transformations. His experience leading teams at scale and his deep expertise across software, hardware, platform and services will be crucial to fulfilling Cruise’s vision of offering technology and services that provide tangible benefits to society. Marc was a founding engineer at Xbox and Xbox Live where he scaled three generations of technology to serve millions of gamers. At Amazon, he was general manager and vice president across a range of entertainment devices and services, including Fire TV, Kindle, and Amazon apps and entertainment services. He served as Chief Product Officer of Sonos and most recently as Chief Product and Technology Officer, Create at Unity, where he helped expand the use of AI and the company’s real-time 3D technology, which powers most real-time games, apps and experiences across multiple platforms. Mo Elshenawy will continue to serve as President and Chief Technology Officer, while Craig Glidden will focus his time as President and Chief Administrative Officer with changes to his leadership role at GM. Both will report to Marc, as will Steve Kenner, Cruise’s Chief Safety Officer. "In a few years, transportation will be fundamentally safer and more accessible than it is today, creating much more value for individuals and communities around the world. It is an opportunity of a lifetime to be part of this transformation," said Marc Whitten on his decision to join Cruise. “The team at Cruise has built world-class technology, and I look forward to working with them to help bring this critical mission to life.” In addition to Marc’s appointment, Cruise also welcomes Nick Mulholland as its Chief Communications & Marketing Officer. He brings deep auto, tech and electric vehicle experience to his role at Cruise. Most recently, he led Rivian’s global communications team including brand marketing, corporate and sustainability communications. “My core belief is that this technology and Cruise's dedication to its mission will change the world. Achieving such a bold vision will require a deep and meaningful collaboration with the communities we serve," said Nick. "It is an incredible opportunity to play a role in building these vital connections." About Cruise In service of its vision for safer roadways, Cruise’s aspiration is to build the world’s most advanced self-driving vehicles to safely connect people with the places, things, and experiences they care about. As of June 2024, Cruise has resumed supervised autonomous driving in Phoenix, AZ, Houston and Dallas, TX, in addition to its ongoing testing in Dubai. Majority owned by General Motors since 2016, Cruise combines a culture of innovative technology and safety with a history of manufacturing and automotive excellence. Cruise has received funding from other leading companies and investors—including Honda, Microsoft, T. Rowe Price, and Walmart.

---

# Steve Kenner outlines safety overhaul for GM Cruise | Automotive News

## 2024-01-01 - archive.is

Source: https://archive.is/2024.08.05-162254/https://www.autonews.com/mobility-report/steve-kenner-outlines-safety-overhaul-gms-cruise

Cruise shut down its nationwide fleet in the aftermath, laid off nearly 1,000 employees and replaced its senior management. It resumed testing, with human safety drivers aboard, in three U.S. "Cruise is a different company today than it was last fall," said Kenner, a longtime safety executive at tech and auto companies who joined Cruise in February. His remarks — the first public comments of his tenure — came July 30 at the Automated Road Transportation Symposium, an annual meeting of business leaders, government officials and researchers focused on self-driving technology. Safety was the prime focus of the gathering as the industry deals with mishaps. Federal safety regulators are conducting ongoing investigations into Cruise, Waymo and Zoox. Moreover, surveys indicate nearly two-thirds of consumers do not trust autonomous vehicles. With a prevailing concern that further incidents could hinder the industry's commercialization, many at the conference said they were encouraged by Cruise's efforts to improve safety. "I'm hopeful for what's happening at Cruise," said Jeffrey Wishart, an engineering professor at Arizona State University and vice president of innovation at the Arizona Commerce Authority. He has helped establish best practices and standards related to automated driving. "Steve is bringing in this new culture, and part of this whole thing is safety culture," Wishart said.

---

# Tesla’s Dojo Supercomputer Head Exits in Blow to Efforts - Bloomberg

## 2023-12-07 - www.bloomberg.com

Source: https://www.bloomberg.com/news/articles/2023-12-07/tesla-s-dojo-super-computer-head-exits-in-blow-to-efforts?leadSource=reddit_wall

To continue, please click the box below to let us know you're not a robot. and cookies and that you are not blocking them from loading. For more information you can review our Terms of Service and Cookie Policy. For inquiries related to this message please contact our support team and provide the reference ID below.

---

# Cruise team updates | Cruise

## 2023-01-01 - www.getcruise.com

Source: https://www.getcruise.com/news/blog/2023/cruise-team-updates/?mod=article_inline

Blog Post 12.14.2023 Cruise Team Updates Share In October, Cruise paused operations to take time to examine our processes, systems and tools and improve how we operate. While we remain committed to commercialization, we will approach it within a thoughtful and achievable time frame—with safety as our north star. As a result of our updated operating plans, today Cruise shared the difficult news that we are making staff reductions impacting 24% of full-time Cruisers. This reflects our new future and a more deliberate go-to-market path, meaning less immediate need for field, commercial operations and corporate staffing. As we look forward, the road to successful commercialization is dependent on defining and meeting an exceptional performance and heightened safety bar. Cruise is committed to playing a key role in defining these standards with the input of our regulators, our communities and other AV industry leaders. We are extremely grateful to the departing employees who have helped further our mission, and the remaining Cruisers who will carry that mission forward in our next chapter. Below is a letter from Mo ElShenawy, President and CTO of Cruise, that went to all employees today: Cruisers: We knew this day was coming, but that does not make it any less difficult—especially for those whose jobs are affected. Today, we are making staff reductions that will affect 24% of full-time Cruisers, through no fault of their own. We are simplifying and focusing our efforts to return with an exceptional service in one city to start with and focusing on the Bolt platform for this first step before we scale. As a result, we are reducing our employee counts in operations and other areas. These impacts are largely outside of engineering, although some Tech positions are impacted also. As you might have learned, yesterday, we took action to part ways with several SLT members. Craig and I believe this is a necessary step, and our leadership team and the board are fully aligned with how our go-forward U.S. staffing needs will map to the priorities ahead of us, and set up Cruise for the long term. We have also ended additional assignments of contingent workers who support our driverless operations, as we refined our go forward plans. In a few moments, you will receive an email letting you know whether or not you are affected by this staffing reduction. If you are impacted, you will get details about what happens next in a subsequent email. Please know that our first priority is to treat departing Cruisers with fairness, and I will describe more about how we are doing that below. I also want to explain why we are making these reductions, and what this means for Cruise moving forward. Cruise today vs Cruise moving forward As we’ve shared, our goal is to focus our work on a fully driverless L4 service that meets a new AV performance bar, prioritize the Bolt platform, relaunch ridehail in one city to start, and enhance our safety standards and processes before we scale. We are ceasing work on the Origin MY24 but not losing sight of our work on future programs. This is very different from our prior plans to expand into more than a dozen new cities in 2024. As a result of our decision to slow down commercialization, we are restructuring to focus on delivering the improvements to our tech and vehicle performance that will build trust in our AVs. Many of you will be impacted because we aren’t commercializing as quickly, and therefore don’t need support in certain cities or facilities. In other cases, we restructured teams based on the work we’re prioritizing. We didn’t take any of these decisions lightly, though I know that isn’t much of a consolation if you’re someone affected by the actions we are taking today. How we’re helping departing employees We know there’s no “good” way to lay off employees, but treating people fairly on their way out was a key principle that guided our approach, and our top priority was determining how we could provide a strong severance package, while treating departing Cruisers with respect. In short, we are offering departing Cruisers pay, at minimum, through April 8, 2024 (approximately 16 weeks), plus continued subsidized health benefits, RSU vesting, the January 5 bonus, and additional immigration support for those holding work visas. Severance details include: Severance pay: Departing employees will remain on payroll through Feb. 12 and are eligible for an additional 8 weeks of pay, with long-term employees offered an additional 2 weeks’ pay per every year at Cruise over 3 years. Bonus: All impacted employees will receive their 2023 bonus (eligible target payout) on Jan. Medical, Dental, Vision: we will provide Cruisers and their dependents who are currently enrolled in Cruise benefits the option to receive Cruise-subsidized medical, dental and mental health/EAP benefits through the end of May. Perks Wallet: We will give Cruisers two months to access the perks most important to them via our Perks Wallet. 401(k): We will give Cruisers two months to continue contributions into their 401(k) plan, including our employer match. RSU vesting: All Cruisers, including those impacted and those remaining, will receive their January 15th RSU vest. In addition, we will provide liquidity for all of these January 15th shares in Q1 based on an updated 409A fair market valuation that we will conduct in the first quarter. Tax obligations for these January 15th vested shares will not be incurred until we provide you liquidity for these shares. Career support: Departing employees will receive a year-long subscription to LinkedIn Premium, and we will create an opt-in alumni directory to connect potential employers with impacted Cruisers. Cruise Talent Acquisition will also run workshops on resume building, networking, and interview prep with departed Cruisers in the new year. Immigration support: We are offering continued time on payroll through March 24 in lieu of a lump-sum severance payment to allow visa holders additional time to help transition and manage their immigration status. Eligibility for the Perks Wallet and 401(k) contributions and match will also continue through this time. We also have dedicated support lined up to help Cruisers based on their needs. Our message to other employers in the market is that each departing Cruiser is a talented, driven, and mission-focused team member who will contribute and achieve great things elsewhere. They are departing us through no fault of their own. Other companies will be privileged to have these professionals on their teams, as we were privileged to have them here during their time at Cruise. What’s next As mentioned, in a few moments, you will receive an email letting you know whether or not you are affected by this staffing reduction, and if you are impacted, you will get details about what happens next. I am so sorry we have to do this by email, as I would prefer that we have a conversation with each of you. Unfortunately, given the scale of this change, this approach allows us to communicate to those who are impacted at the same time. We know you will want to say goodbye to your colleagues, so you will have access to Cruise email and Zoom for the next couple of hours (until 10am PT). This is one of the hardest days we’ve had so far because so many talented people are leaving. I’m thankful we had the chance to work together, and I know I speak on behalf of so many Cruisers who will be reaching out to those departing to help with our professional networks and references. On behalf of the SLT, the Cruise Board and GM, I’m truly grateful to everyone who has played a role in building Cruise and who has poured so much into the promise of making our roads safer and our world better

---

# Autonomous Vehicle Sensor Technology Development | Cruise

## 2020-01-01 - www.getcruise.com

Source: https://www.getcruise.com/news/blog/2020/how-cruise-uses-simulation-to-speed-up-our-sensor-development/

Blog Post 10.1.2020 How Cruise Uses Simulation To Speed Up Our Sensor Development Share In January of this year, we introduced the Cruise Origin — the first purpose-built, fully driverless vehicle with a million-mile lifespan. And it’s what you’d build if you could start from scratch to create a transportation system that’s safer, better and more affordable for us, for our cities and for our planet. The importance of our vision is why, even during a pandemic, we haven’t slowed down. General Motors continues to progress on development of the all-electric propulsion system for the Origin, with the first propulsion system test vehicle already on the roads of one of their massive test facilities in Michigan. Here in San Francisco, we’ve kept moving forward on AV development using our robust simulation frameworks thanks to the years of data accumulated from on-road testing in one of the most complex city environments in the world. But that’s not the only cool way we’re using simulation. We’ve also built a bespoke Sensor Placement Tool that uses simulation to help select and place the sensors on the Origin well before the vehicle goes into production. This tool is key to Origin development because it helps ensure overlapping sensor coverage in 360 degrees around the Cruise vehicle — giving us eyes not only in the back of our heads, but on the sides as well. This will enable the Origin to safely perform difficult maneuvers, such as driving around large double parked vehicles while detecting other roadway users, pedestrians and debris. Placing and evaluating sensors Cruise’s Sensor Placement Tool accurately enables the virtual placement, evaluation, and iteration of various sensor layouts across three sensor types: camera, radar, and LiDAR. We have accurately modeled the camera’s field of view and distortion, radar field of view, range and point cloud distribution, and LiDAR beam distribution and intensity. These simulated sensors have been refined with rigorous physical evaluation of real world sensor data to cover any unexpected anomalies. The Sensor Placement Tool also supports the proper offset of the actual optical emission for each sensor from their planned physical housing. This ensures that the virtual sensors mounted in our Sensor Placement Tool are as accurately positioned as possible to their physical, hardware counterparts. Lastly, we utilize a hardware-accurate CAD-based model of the Origin and load it into the Sensor Placement Tool for frame of reference during placement. This provides proper self occlusions to ensure the most physically accurate representation of the new sensor placement we are testing, including unexpected occlusions with the Origin vehicle body and other sensors such as camera field of view occlusion to radar. Hardware Development without the Hardware: Allowing for Modular Configuration With our refined Sensor Simulation, CAD-based Origin, and Sensor Placement Tool, we are able to unlock dynamic modular iteration of the Origin vehicle to greatly accelerate AV development. This functionality enables us to quickly test several sensor configurations and layouts in our tools, which would otherwise take weeks or months with actual hardware. With tight collaboration and review with our Cruise Hardware team, we are able to confidently iterate rapidly with various Origin layouts. As always, safety is our primary concern. The ability to verify our dual modality/sensor type coverage at any given point with our systems helps ensure our sensor placement is not only performant in terms of maneuvers, but for safety coverage as well. Using simulation, we are able to test sensors that don’t yet exist in completed hardware form. We can also assess various configurations of existing sensors to evaluate for better modality coverage or occlusion clearance. Examples of what we can assess include: Trying cameras with greater or different fields of view or better coverage of a unique area around the Origin while avoiding occlusion from another sensor Different radar placement for driving at highway speed LiDARs with different beam patterns and unique orientations to try different configuration layouts for better coverage. Being able to develop the sensor placement for the Origin virtually in our Sensor Placement Tool significantly expedites development by enabling early decision making for sensor selection, placement, and vehicle design choices without relying on physical hardware. This saves time, lowers costs, and adds flexibility to the Cruise Origin development process. Optimizing Sensor Calibration using Sensor Simulation Evaluating and placing sensors is only one part of the production process. Once we have selected a prospective sensor setup, we also need to consider how to calibrate that sensor on the Origin before it hits the road. Sensor calibration confirms the sensor’s actual position and orientation on the vehicle — AKA extrinsics — which might actually be slightly different than the planned extrinsic placement for the vehicle. Small differences in each sensor due to manufacturing, shipping, placement, and even jostling on the road can cause differences in the expected vs. actual extrinsics of each sensor. With an accurate and efficient calibration process, we can ensure the Origin will have an effective sensor fusion: combining information from different sensors. Knowing relative position and orientation helps us most accurately combine data. However, performing calibration is complex and involves an array of fixtures with special markings that enable the calibration system to determine the actual intrinsic (internal sensor configuration) and extrinsic parameters by solving several optimization problems programmatically. Placement of the fixtures is key and varies for each sensor modality/type. Determining optimal placement for calibration setup can be very time consuming, and once there is a change to sensor layout, that calibration environment must be adjusted as well. Since the Origin can support a variety of sensor layouts, refining this process is paramount to efficient development. Simulation-accelerated Calibration Setup Using simulation frameworks, our engineers are able to evaluate calibration setups needed to determine proper calibration. This is all done on a computer in a fraction of the time that it would take to reposition the various fixtures manually. Once a new calibration method is determined in simulation, it can be built out in real life. Not only is this a faster approach for developing and testing, it’s also potentially less error prone. This is because starting from a simulated environment ensures that the Origin will have perfect calibration by calibrating to a known good simulation setup as opposed to starting from a setup Origin vehicle, which will inherently have some minor calibration offsets (as all manufactured parts do). Leveraging simulation, various calibration errors can be tested by forcing faulty calibrations into our virtual environment. The calibration system then can be tested for proper determination of the new calibration parameters, thereby further ensuring the best calibration system for our Origin vehicles. Finally, given that the Origin’s sensor layout is subject to change throughout the development cycle, using simulation frameworks enables us to have accurate and efficient calibration updates to keep up with these changes. Designing the future of transportation Simulation frameworks accelerate the development of the Cruise Origin by enabling engineers to determine the sensor selection and placement faster than the weeks or months it would take with actual hardware. Simulation also significantly reduces the costs associated with producing physical hardware while adding flexibility to the Cruise Origin development process. The sooner we can safely bring the Cruise Origin to the roads of San Francisco, the faster we will be able to deliver the benefits of all-electric, self-driving transportation to our cities: improving safety by removing the human driver, reducing emissions by being all-electric, and alleviating congestion by making shared rides an awesome experience at a radically lower cost. Utilizing simulation frameworks to expedite the Origin’s production is just one of the complex challenges Cruise is solving. Join our simulation, hardware, and AI teams to create the future of transportation.

---

# Self-driving vehicles can keep us safer on the roads | Cruise

## 2020-01-01 - www.getcruise.com

Source: https://www.getcruise.com/news/blog/2020/cruises-continuous-learning-machine-predicts-the-unpredictable-on-san/

Blog Post 9.10.2020 Cruise’s Continuous Learning Machine Predicts the Unpredictable on San Francisco Roads Share The promise of all-electric, self-driving vehicles is monumental: they will keep us safer on the road, help make our air cleaner, transform our cities, and give us back time. To make this possible, we are working towards developing an autonomous vehicle (AV) which, among other things, is able to understand and predict the world around it at a superhuman scale. This is the engineering problem of our generation and the cutting edge of AI. One of the core challenges of autonomous vehicles is accurately predicting the future intent of other people and cars on the road to make the correct decisions. Will that pedestrian cross the road in front of us? Is that car about to cut us off? Will the car in front of us run the yellow light or hit the brakes? People don’t necessarily follow the rules of the road, and answers to these questions are often quite fuzzy, which is why we take a machine-learning-first approach to prediction. Our machine learning prediction system has to generalize to both completely nominal events as well as events that it sees very infrequently (“long tail” events). Examples of long tail situations include vehicles performing U-turns or K-turns, bicycles swerving into traffic, or pedestrians unexpectedly running into the street. The biggest challenge with prediction is not building a system that is able to handle the nominal cases well, but rather building something that generalizes well to uncommon situations and does so accurately. Predicting the long tail requires data One important task is to ensure that rare events are sufficiently represented in the dataset. As with any ML system, a model will only scale as well as its training data, so addressing dataset imbalance must be a primary focus. Once we are able to expose our model to sufficient training examples, it should be able to learn to predict long tail situations effectively. It is important to note that these long tail events do exist in the data we see on the road, but they are rare and infrequent. As a result, we focus on finding the needle in the haystack of regular driving and use upsampling to teach the models more about these events. A naive approach to identifying rare events is to hand engineer “detectors” for each of these long tail situations to help us sample data. For example, we could write a “u-turn” detector and sample scenarios any time it triggers. This approach would help us collect targeted data, but quickly breaks down when it comes to solving for scale because it’s impossible to write a detector specific enough for each unique longtail situation. Insight 1: Prediction data can be auto-labeled using our perception stack At its core, a predicted trajectory is a representation of what we believe someone is going to do in the future. When we possess the ability to observe what that person does in the future, evaluating a predicted trajectory becomes straightforward by comparing it against the observed trajectory. Said another way, future perception output can be compared against our current predictions to create a self-supervised learning framework. This enables Cruise engineers to use our perception stack to label data for our prediction models. Imagine that our AV predicts that the car in front of it will turn left in the upcoming intersection. After observing the car, we see that they did not turn left, but rather proceeded straight through the intersection instead. By comparing the AV’s prediction (a trajectory that turns left) with its future perception (a trajectory that goes straight), our engineers can easily create labels for our prediction without requiring any human input. In this example, the model incorrectly predicts the vehicle to follow a trajectory that turns left, but we can identify the correct trajectory as one that goes straight. This is in direct contrast to many other ML tasks, such as object detection, where human annotators are commonly used to label “ground truth”. Human annotation is time-consuming, error-prone, and/or expensive. In contrast, we auto-label our data using observation of future behavior from our perception system. Insight 2: Auto-labeling data allows for automated failure mining The self-supervised framework described above for identifying an incorrect left turn trajectory enables the creation of an active learning framework where error situations can be automatically identified and mined. To build diverse, high coverage datasets — and in turn, high quality models — our goal is to upsample data from any and all error scenarios, not just ones that have explicit detectors available for them. An auto-labeled approach facilitates maximum coverage in our dataset because it is able to identify and mine all errors from our models, ensuring that no valuable data is missed. It also keeps the dataset as lean as possible by ensuring that we don’t add more data for scenarios that are already solved. The self-supervised framework for auto-labeling data combined with an active learning data mining framework creates Cruise’s home-grown Continuous Learning Machine (CLM), an entirely self-serving loop that addresses the data sampling challenge and scales to meet even the most challenging longtail problems. Cruise’s Continuous Learning Machine (CLM) solves long tail prediction problems Below is a break down of what each section of the above diagram represents: Drives: The CLM starts with our fleet navigating downtown San Francisco where our vehicles are exposed to challenging situations 46 times more frequently than in suburban environments. Despite this, long tail maneuvers such as U-turns comprise less than 0.1% of our observed data. Error Mining: Utilizing active learning, error cases are automatically identified and only scenarios where we see a significant difference between prediction and reality are added to the dataset. This enables extremely targeted data mining, ensuring we add only high-value data and avoid bloating our datasets with a copious amount of “easy” scenarios. Labeling: The self-supervised framework labels all of our data automatically using future perception output as “ground truth” for all prediction scenarios. The core CLM structure extends to other ML problems where a human annotator can fill in, but fully automating this step within prediction enables significant scale, cost, and speed improvements that allow this approach to truly span the entirety of the long tail. Model Training and Evaluation: The final step is to train a new model, run it through extensive testing, and deploy it to the road. Our testing and metrics pipelines ensure that a new model exceeds the performance of the previous model and generalizes well to the nearly infinite variety of scenarios found in our test suites. Cruise has invested significantly in our ML infrastructure, which allows a variety of time-consuming steps to be heavily automated. As a result, we can now create an entire CLM loop without requiring human intervention. Putting the Continuous Learning Machine into practice Below is a series of examples that showcases a variety of scenarios our models experience infrequently but have been exposed to enough within the dataset to be able to make an accurate prediction. U-turns Human drivers may encounter another motorist making an abrupt (and sometimes illegal) U-turn sporadically. Even with our cars navigating the streets of San Francisco at all times, we see less than 100 U-turns each day on average, making U-turns an ideal candidate for the CLM. Below are a few examples of U-turns that our AVs encounter. Example 1 represents a busy intersection where all vehicle trajectories that leave the intersection via the top right exit lane are highlighted in red. We see that most of the dataset contains drivers moving straight with a small number of left turns, an even smaller number of lane changes, and only two U-turn trajectories. We also see examples of mid-block U-turns (Example 2), as well as intersections — but again, they are very sparse in our data. Applying our CLM principles, an initial deployment of our model may not optimally predict U-turn situations. As a result, when we sample data, error situations commonly include U-turns. Over time, our dataset steadily grows in representation of U-turns until the model can sufficiently predict them and the AV can ultimately navigate these scenarios accurately. In the video above (Example 3), the cyan-blue line shows that the model recognizes the vehicle in front of us is in the early stages of a U-turn. The vehicle has barely entered the intersection, but the model has been exposed to enough examples within the dataset to accurately make this prediction. K-turns The K-turn is a three-point turn where the driver must maneuver both forwards and in reverse in order to turn around into the opposite direction. These are uncommon and most often used when the street is too narrow for a U-turn, meaning we see examples approximately half as frequently. Example 4 represents what a K-turn trajectory looks like in our data as a vehicle completes a K-turn, then immediately pulls into a parking spot on the other side of the road. Again, despite seeing K-turns so rarely, we are able to predict this direction change mid-maneuver. In the video below (Example 5), the car in front of us is perpendicular to the lane and moving backwards as it completes the second stage of a K-turn. Following the cyan blue predicted trajectory, the prediction for this vehicle is first to continue moving backwards, then pause and drive forward while turning into the correct direction to follow the nearby lane. Cut-ins Another interesting phenomenon we observe is people changing their trajectory for slowing or stationary objects ahead of them. This often leads to people cutting in front of nearby cars as they attempt to move around the object in front of them. Example 6 represents an example trajectory from our dataset. We see the vehicle move over into the next lane temporarily, then return to their lane once they have passed the obstacle. Cut-ins are common in both vehicles and bicycles. For instance, Example 7 depicts a before-and-after following a round of data mining. On the left, the bicycle trajectory passes through the stationary parked car — which is an unrealistic trajectory. After a round of data mining, we get the trajectory on the right where we are now able to predict the bike to move around the parked car instead. In the video below (Example 8), there is a bus stopped in the right lane at a bus stop, which encourages the car behind it to overtake it. This car then cuts in front of the Cruise AV very suddenly. Situations like this require us to predict such behavior early so that the Cruise AV can slow down preemptively and give this car enough space to go around. The future of ML is in Autonomous Vehicles Despite the challenge of predicting the long tail behaviors of others on the road, building a scalable ML solution is very achievable. The problems we are solving in AV, such as generalizing across the long tail, are at the core of machine learning.While this blog discussed prediction primarily as a single agent problem, e.g. “what will that specific object do next”, prediction is actually a multi-agent problem. We need to understand both the intent of other agents on the road, and reason about the sequence and interactions between different agents and how they will evolve over time. The complexity of this problem is it’s own field of research, which is another reason why autonomous vehicles are the greatest engineering challenge of our generation.The future of ML is happening in Cruise. Join us if you’re interested in pushing ML across the next frontier.

---

# Self-driving cars can navigate double parked cars | Cruise

## 2019-01-01 - www.getcruise.com

Source: https://www.getcruise.com/news/blog/2019/how-self-driving-cars-think-navigating-double-parked-vehicles/

Blog Post 6.27.2019 How Self-Driving Cars Think: Navigating Double-Parked Vehicles Share Every day, San Franciscans drive through six-way intersections, narrow streets, steep hills, and more. While driving in the city, we check mirrors, follow the speed limit, anticipate other drivers, look for pedestrians, navigate crowded streets, and more. For many of us who have been driving for years, we do these so naturally, we don’t even think about it. At Cruise, we’re programming hundreds of cars to consider, synthesize, and execute all these automatic human driving actions. In SF, each car encounters construction, cyclists, pedestrians, and emergency vehicles up to 46 times more frequently than in suburban environments, and each car learns how to maneuver around these aspects of the city every day. To give you an idea of how we’re tackling these challenges, we’re introducing a “How Self-Driving Vehicles Think” series. Each post will highlight a different aspect of teaching our vehicles to drive in one of the densest urban environments. In our first edition, we’re going to discuss how our Cruise self-driving vehicles handle double-parked vehicles (DPVs). How Cruise autonomous vehicles maneuver around double-parked vehicles Every self-driving vehicle “thinks” about three things: Perception: Where am I and what is happening around me? Planning: Given what’s around me, what should I do next? Controls: How should I go about doing what I planned? One of the most common scenarios we encounter — that requires the sophisticated application of all three of these elements — is driving around double-parked vehicles. On average in San Francisco, the probability of encountering a double-parked vehicle is 24:1 compared to a suburban area. The Cruise fleet typically performs anywhere between 200 to 800 oncoming maneuvers each day! Since double-parked vehicles are extremely common in cities, Cruise cars must be equipped to identify and navigate around them as part of the normal traffic flow. Perception Recognizing whether a vehicle is double-parked requires synthesizing a number of cues at once, such as: How far the vehicle is pulled over towards the edge of the road The appearance of brake and hazard lights The last time we saw it move Whether we can see around it to identify other cars or obstacles How close we are to an intersection We also use contextual cues like the type of vehicle (i.e. delivery trucks, who double-park frequently), construction activity, and scarcity of nearby parking. To enable our cars to identify double-parked vehicles, we collect the same information as humans. Our perception software extracts what cars around the Cruise autonomous vehicle (AV) are doing using camera, lidar, and radar images: Cameras provide the appearance and indicator light state for vehicles, and road features (such as safety cones or signage) Lidars provide distance measurements Radars provide speeds All three sensors contribute to identifying the orientation and type of vehicle. Using advanced computer vision techniques, the AV processes the raw sensor returns to identify discrete objects: “human,” “vehicle,” “bike,” etc. By tracking cars over time, the AV infers which maneuver the driver is making. The local map provides context for the scene, such as parking availability, the type of road, and lane boundaries. But to make the final decision — is a car double-parked or not — the AV needs to weigh all these factors against one another. This task is perfectly suited for machine learning. The factors are all fed into a trained neural network, which outputs the probability that any given vehicle is double-parked. In particular, we use a recurrent neural network (RNN) to solve this problem. RNNs stand out from other machine-learning implementations because they have a sense of “memory.” Each time it is rerun (as new information arrives from the sensors), the RNN includes its previous output as an input. This feedback allows it to observe each vehicle over time and accumulate confidence on whether it is double-parked or not. Planning & Controls Getting from A to B without hitting anything is a pretty well known problem in robotics. Comfortably getting from A to B without hitting anything is what we work on in the Planning and Controls team. Comfortable isn’t just defined by how quickly we accelerate or turn, it also means behaving like a predictable and reasonable driver. Having a car drive itself means we need our vehicles’ actions to be easily interpretable by the people around us. Easy-to-understand (i.e. human-like) behavior in this case comes from identifying DPVs and reacting to them in a timely manner. Once we know that a vehicle in front of us is not an active participant in the flow of traffic, we can start formulating a plan to get around the vehicle. Often times, we try to lane change around or route away from the obstacle. If that is not possible or desirable, we try to generate a path that balances how long we are in an oncoming lane with our desire to get around the DPV. Every time the car plans a trajectory around a double-parked vehicle, the AV needs to consider where the obstacle is, what other drivers are doing, how to safely bypass the obstacle, and what the car can and cannot perceive. A Cruise AV navigates around a double-parked truck in the rain, with other vehicles approaching in the oncoming lane. The AV yields right-of-way to two vehicles, which in turn are going around a double-parked vehicle in their own lane. Every move we plan takes into account the actions of the road users around us, and how we predict they will respond to our actions. With a reference trajectory planned out, we are ready to make the AV execute a maneuver. There are many ways to figure out the optimal actions to perform in order to execute a maneuver (for example, Linear Quadratic Control); however, we also need to be mindful of the constraints of our vehicle, such as how quickly we can turn the steering wheel or how quickly the car will respond to a given input. To figure out the optimal way to execute a trajectory given these constraints, we use Model Predictive Control (MPC) for motion planning. Under the hood, MPC algorithms use a model of how the system behaves (in this case, how we have learned the world around us will evolve and how we expect our car to react) to figure out the optimal action to take at each step. Finally, these instructions are sent down to the controllers, which govern the movement of the car. Putting it all together, we get: After yielding to the cyclist, we see an oncoming vehicle allowing us to complete our maneuver around the double-parked truck. It is important to recognize these situations and complete the maneuver so we support traffic flow. San Francisco is famously known to be difficult to drive in, but we at Cruise cherish the opportunity to learn from the city and make it safer. With its mid-block crosswalks, narrow streets, construction zones, and steep hills, San Francisco’s complex driving environment allows us to iterate and improve quickly, so we can achieve our goal of making roads safer. Over the coming months, we look forward to sharing more “How Self-Driving Vehicles Think” highlights from our journey. If you’re interested in joining engineers from over 100 disciplines who are tackling one of the greatest engineering challenges of our generation, join us.

---

# Former Cruise partner fires back in fight over GM deal

## 2016-04-15 - www.freep.com

Source: https://www.freep.com/story/money/cars/general-motors/2016/04/15/former-cruise-partner-fires-back-fight-over-gm-deal/83073480/

Former Cruise partner fires back in fight over GM deal Ex-partner wants share of reported $1B GM is paying to acquire Silicon Valley firm. The battle over who gets how much of the reported $1-billion General Motors is paying to acquire a San Francisco self-driving startup escalated late Thursday as Cruise Automation's previously silent partner filed a countersuit to get his share of the proceeds. Jeremy Guillory says he was indeed a 50% owner of Cruise with cofounder and current CEO Kyle Vogt, based on an October 2013 application for funding from Y Combinator, a Mountain View, Calif., seed capital investment fund. He wants a jury trial to determine his share based on an accounting of Cruise's financial records and documents related to the GM deal. GM declines to say how much it paid, but several media outlets have said it is more than $1 billion, based on sources in the Silicon Valley venture capital community. GM's Cruise deal triggers Silicon Valley lawsuit Last week, Vogt and Cruise sued Guillory, alleging he left the company shortly after it was incorporated and he didn't hold shares or invent or patent any technologies or hardware in the system that attracted GM's interest. What attracted GM was a device Cruise employees have created that enables drivers to take a car into traffic, then push a button that transfers control of the accelerator, brakes and steering. The company also has worked on autonomous features on tractors and mining equipment. Some of the highlights disclosed in Guillory's response include: - He developed a 3D Light Detection and Ranging (LIDAR) sensor system for a company called Graymatter Inc. that sponsored an entry in a 2005 government-sponsored challenge for self-driving cars from some of the country's elite engineering universities. - Guillory depicts himself as the technical leader and Vogt as the fund-raising expert. - The only documentation showing a 50/50 ownership split between Vogt and Guillory was the application to Y Combinator. - While there was an apparent agreement of the 50/50 ownership, Vogt was the only director of the company. Vogt also agreed to contribute a $100,000 convertible note that, if converted, would give him a majority ownership stake. - Vogt told Guillory in November 2013 he no longer wanted to work with him. - After GM announced it would acquire Cruise on March 11, Guillory called Vogt asking how much of the selling price he would receive. - Vogt first said Guillory would receive nothing, but then made offers of $100,000, $1 million and $1.5 million. - About a week after the GM announcement, Sam Altman, president of Y Combinator, which helped Cruise raise $12.5 million last year, offered Guillory $4.5 million from the GM deal if he would sign an agreement that same day, which Guillory declined to do. GM and Cruise intend to close the acquisition by the end of June, but that could be delayed if this Cruise fight goes to trial. While partnerships are developing between automakers and software companies, this deal represents one of the first complete acquisitions of a Silicon Valley startup by a traditional automaker. The race to develop fully autonomous vehicles has picked up in the last year. Most of the auto industry, as well as Google and Uber, are investing heavily and working with federal and state regulators to reach a standardized set of safety and other guidelines to ease the path to market for self-driving cars. Unrelated to the Cruise acquisition GM will launch a fleet of autonomous Chevrolet Volts at its Warren Technical Center this summer for use by employees. Cruise has about 40 employees and is trying to hire more. GM will report its first-quarter financial results next Thursday and analysts likely will be asking about the impact of any delay in the Cruise acquisition. Contact Greg Gardner: 313-222-8762 or ggardner@freepress.com. Follow him on Twitter @GregGardner12

---

